{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGWWKf1Ku4WxOET22Rq+KP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsatrier/precog/blob/main/task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "## Model Used\n",
        "\n",
        "I used a YOLOv8 model and trained it on the COCO dataset for performing object detection on the hateful memes dataset."
      ],
      "metadata": {
        "id": "EUJumIFgWHQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk1SyzvdXQlY",
        "outputId": "37bf0a2b-0643-47a3-e8e3-5ee5cb3ee979"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.1.15-py3-none-any.whl (715 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.1/715.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\" #had an issue with encoding"
      ],
      "metadata": {
        "id": "js94_COObRLU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/parthplc/facebook-hateful-meme-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKWU3SOfbdRw",
        "outputId": "bd97d035-e320-497f-e8d5-d6485adf4dcc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aadid04\n",
            "Your Kaggle Key: ··········\n",
            "Downloading facebook-hateful-meme-dataset.zip to ./facebook-hateful-meme-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.35G/3.35G [02:55<00:00, 20.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "xRbue8cOZUx8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwshSpJ8aXYH",
        "outputId": "90b2bdfb-494f-422e-aa2c-fd0c4985918d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 249MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.train(data='coco8.yaml', epochs=100, imgsz=640)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pveS83xXZ9NU",
        "outputId": "1be5d708-6533-47a3-e94d-9c8a9cc0fd0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.15 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=100, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433k/433k [00:00<00:00, 47.9MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100%|██████████| 25/25 [00:00<00:00, 3611.17file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success ✅ (2.6s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 110MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 284.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 7486.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      1/100      0.78G     0.9308      3.155      1.291         32        640: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.858       0.54      0.727       0.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      2/100     0.793G      1.162      3.127      1.518         33        640: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.904      0.526      0.734      0.497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      3/100     0.782G      0.925      2.507      1.254         17        640: 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.906      0.532      0.741      0.513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      4/100     0.782G      1.131      2.586      1.403         27        640: 100%|██████████| 1/1 [00:00<00:00,  3.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.609      0.714      0.783      0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      5/100      0.82G      1.293      2.959      1.641         25        640: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.611      0.717      0.778      0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      6/100      0.82G      1.183      2.878      1.472         23        640: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.618      0.725      0.749      0.526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      7/100      0.82G      1.224      3.258       1.58         31        640: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.909       0.55       0.75      0.544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      8/100      0.82G      1.333       2.35      1.586         29        640: 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.915       0.55       0.75      0.542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      9/100     0.816G      1.125      3.881      1.704         14        640: 100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.92       0.55       0.75      0.528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     10/100      0.82G     0.8626      2.278       1.33         30        640: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 15.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.925       0.55      0.751      0.528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     11/100      0.82G     0.8773      2.015      1.254         21        640: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.923       0.55      0.837      0.565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     12/100     0.818G      1.173      1.984      1.356         34        640: 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.926       0.55      0.834      0.554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     13/100      0.82G      1.236      2.902      1.596         25        640: 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.942      0.535      0.836      0.552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     14/100     0.818G      1.298      2.124      1.569         18        640: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.944      0.536      0.661      0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     15/100      0.82G      1.071      2.793      1.417         25        640: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.934       0.55      0.661      0.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     16/100      0.82G      1.031      1.686      1.367         31        640: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.895       0.55      0.661      0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     17/100      0.82G     0.8749      1.674      1.297         23        640: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.872       0.55      0.661      0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     18/100      0.82G      1.069      2.518      1.356         39        640: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.872       0.55      0.661      0.509\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     19/100      0.82G      1.005      2.312      1.419         29        640: 100%|██████████| 1/1 [00:00<00:00,  7.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.813      0.554      0.634      0.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     20/100      0.82G      0.993      2.409      1.473         24        640: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.813      0.554      0.634      0.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     21/100      0.82G     0.8296      1.687      1.308         20        640: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.792      0.561      0.632      0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     22/100      0.82G     0.9675      1.818      1.395         22        640: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.792      0.561      0.632      0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     23/100     0.822G     0.9514      2.097      1.265         41        640: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.762      0.583      0.632      0.481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     24/100      0.82G      1.095      1.825      1.445         37        640: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.762      0.583      0.632      0.481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     25/100      0.82G      1.069      1.435      1.351         22        640: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.725      0.583      0.631      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     26/100      0.82G     0.9423      1.801      1.376         20        640: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.725      0.583      0.631      0.474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     27/100     0.822G     0.9536       1.86      1.311         37        640: 100%|██████████| 1/1 [00:00<00:00,  7.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.94      0.383       0.62      0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     28/100      0.82G      0.805      1.698      1.212         34        640: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.94      0.383       0.62      0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     29/100      0.82G       1.08      1.699      1.423         35        640: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.943      0.383      0.613      0.471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     30/100      0.82G     0.7429      2.063      1.218         26        640: 100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.943      0.383      0.613      0.471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     31/100      0.82G      0.895       1.43        1.4         20        640: 100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.942      0.375      0.602      0.448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     32/100     0.816G      1.152       2.28      1.396         15        640: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.942      0.375      0.602      0.448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     33/100      0.82G     0.7457      1.332      1.154         34        640: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.94      0.375      0.578      0.445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     34/100      0.82G     0.8135      1.697      1.274         31        640: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.94      0.375      0.578      0.445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     35/100      0.82G     0.6167       1.44      1.089         36        640: 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.939      0.375      0.578      0.445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     36/100      0.82G     0.7441      1.123      1.101         33        640: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 13.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.939      0.375      0.578      0.445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     37/100     0.818G      1.068      1.791      1.575         15        640: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.935      0.374      0.576      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     38/100      0.82G     0.8777      1.086      1.215         26        640: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.935      0.374      0.576      0.446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     39/100     0.814G     0.5675      1.173      1.105         12        640: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.931      0.374      0.578      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     40/100     0.818G     0.6878      0.863      1.087         30        640: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.931      0.374      0.578      0.457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     41/100      0.82G     0.9914      1.683      1.407         31        640: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.764      0.373      0.577      0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     42/100      0.82G     0.5834     0.7123     0.9234         27        640: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.764      0.373      0.577      0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     43/100      0.82G     0.8912      1.159       1.23         35        640: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 13.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.764      0.374      0.495      0.368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     44/100      0.82G      0.794      1.287      1.176         31        640: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.764      0.374      0.495      0.368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     45/100      0.82G     0.8126     0.9474       1.24         39        640: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.767      0.373      0.494      0.402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     46/100     0.816G     0.7662      1.092      1.169         18        640: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 15.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.767      0.373      0.494      0.402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     47/100      0.82G     0.7026     0.8885      1.195         20        640: 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.77      0.373      0.496      0.401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     48/100      0.82G     0.8677      1.564      1.301         19        640: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 15.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17       0.77      0.373      0.496      0.401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     49/100      0.82G     0.8949       1.14      1.324         29        640: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.768      0.373      0.497      0.401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     50/100      0.82G     0.7853      1.326      1.256         24        640: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 12.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.768      0.373      0.497      0.401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     51/100     0.822G     0.8446     0.9784      1.167         41        640: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.769      0.373      0.497      0.383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     52/100     0.816G      1.073      1.813      1.506         19        640: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.769      0.373      0.497      0.383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     53/100     0.818G     0.9789     0.9341      1.206         40        640: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.769      0.373      0.497      0.383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     54/100      0.82G     0.8291      1.492      1.259         31        640: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.944      0.373      0.472      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     55/100     0.818G     0.5788      1.515       1.04         24        640: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.944      0.373      0.472      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     56/100     0.822G     0.7711      1.378      1.192         31        640: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 14.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.944      0.373      0.472      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     57/100     0.816G     0.6254      1.025      1.094         18        640: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.945      0.375      0.449      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     58/100     0.816G     0.8839      1.145      1.377         23        640: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.945      0.375      0.449      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     59/100     0.816G     0.4829       1.21      1.157         14        640: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 17.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.945      0.375      0.449      0.351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     60/100      0.82G     0.5725     0.9717       1.11         24        640: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.943      0.383      0.444      0.342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     61/100      0.82G     0.9639      1.593      1.342         44        640: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 16.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.943      0.383      0.444      0.342\n",
            "Stopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 11, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "61 epochs completed in 0.013 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.15 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00, 18.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.922       0.55      0.837      0.554\n",
            "                person          4         10      0.981        0.3      0.484      0.246\n",
            "                   dog          4          1          1          0      0.995      0.436\n",
            "                 horse          4          2      0.941          1      0.995      0.698\n",
            "              elephant          4          2          1          0      0.559     0.0559\n",
            "              umbrella          4          1      0.765          1      0.995      0.995\n",
            "          potted plant          4          1      0.845          1      0.995      0.895\n",
            "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import cv2"
      ],
      "metadata": {
        "id": "F4b-A54sb3aC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd facebook-hateful-meme-dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBoNhTgbdsr6",
        "outputId": "3aa202eb-e26e-4542-c636-74940e5c8666"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/facebook-hateful-meme-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/train.jsonl', 'r') as f:\n",
        "    json_lines = f.readlines()"
      ],
      "metadata": {
        "id": "zI-YPOILdKoo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpdlWUbrfM3F",
        "outputId": "715b4876-4b33-4a33-8e51-55d99a05e3ca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/facebook-hateful-meme-dataset/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = {}\n",
        "img_count = 0\n",
        "for line in json_lines:\n",
        "\n",
        "    img_count+=1\n",
        "    print(img_count)\n",
        "    # Parse JSON from the line\n",
        "    data = json.loads(line)\n",
        "    # Extract image path\n",
        "    img_path = data[\"img\"]\n",
        "    print(img_path)\n",
        "\n",
        "    # Load image using OpenCV\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Perform object detection using YOLOv8\n",
        "    results = model.predict(source=img, save=True, save_txt=True)\n",
        "\n",
        "    result = results[0]\n",
        "\n",
        "    for i in range(0, len(result.boxes)):\n",
        "      box =  result.boxes[i]\n",
        "      label = result.names[box.cls[0].item()]\n",
        "\n",
        "      if label in freqs:\n",
        "          freqs[label] += 1\n",
        "      else:\n",
        "          freqs[label] = 1\n",
        "    if img_count == 4000:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t3juQ0jKdx0O",
        "outputId": "efd2d3c2-12bf-423b-b259-86f7a1c98952"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Results saved to \u001b[1mruns/detect/train3286\u001b[0m\n",
            "1 label saved to runs/detect/train3286/labels\n",
            "3287\n",
            "img/07539.png\n",
            "\n",
            "0: 640x448 1 person, 1 bottle, 20.5ms\n",
            "Speed: 2.5ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3287\u001b[0m\n",
            "1 label saved to runs/detect/train3287/labels\n",
            "3288\n",
            "img/48925.png\n",
            "\n",
            "0: 448x640 1 person, 19.6ms\n",
            "Speed: 2.6ms preprocess, 19.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3288\u001b[0m\n",
            "1 label saved to runs/detect/train3288/labels\n",
            "3289\n",
            "img/51309.png\n",
            "\n",
            "0: 512x640 3 persons, 19.3ms\n",
            "Speed: 3.5ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3289\u001b[0m\n",
            "1 label saved to runs/detect/train3289/labels\n",
            "3290\n",
            "img/94021.png\n",
            "\n",
            "0: 448x640 31 hot dogs, 19.7ms\n",
            "Speed: 2.6ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3290\u001b[0m\n",
            "1 label saved to runs/detect/train3290/labels\n",
            "3291\n",
            "img/96342.png\n",
            "\n",
            "0: 640x512 1 person, 1 tie, 20.1ms\n",
            "Speed: 3.6ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3291\u001b[0m\n",
            "1 label saved to runs/detect/train3291/labels\n",
            "3292\n",
            "img/57821.png\n",
            "\n",
            "0: 448x640 (no detections), 20.5ms\n",
            "Speed: 3.2ms preprocess, 20.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3292\u001b[0m\n",
            "0 label saved to runs/detect/train3292/labels\n",
            "3293\n",
            "img/93572.png\n",
            "\n",
            "0: 448x640 1 cup, 1 spoon, 1 bowl, 18.6ms\n",
            "Speed: 2.6ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3293\u001b[0m\n",
            "1 label saved to runs/detect/train3293/labels\n",
            "3294\n",
            "img/73604.png\n",
            "\n",
            "0: 448x640 1 surfboard, 4 bottles, 20.2ms\n",
            "Speed: 3.2ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3294\u001b[0m\n",
            "1 label saved to runs/detect/train3294/labels\n",
            "3295\n",
            "img/89543.png\n",
            "\n",
            "0: 640x288 4 persons, 18.8ms\n",
            "Speed: 2.9ms preprocess, 18.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 288)\n",
            "Results saved to \u001b[1mruns/detect/train3295\u001b[0m\n",
            "1 label saved to runs/detect/train3295/labels\n",
            "3296\n",
            "img/42759.png\n",
            "\n",
            "0: 640x640 1 person, 19.6ms\n",
            "Speed: 3.3ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3296\u001b[0m\n",
            "1 label saved to runs/detect/train3296/labels\n",
            "3297\n",
            "img/52783.png\n",
            "\n",
            "0: 640x448 1 person, 3 cakes, 19.7ms\n",
            "Speed: 2.8ms preprocess, 19.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3297\u001b[0m\n",
            "1 label saved to runs/detect/train3297/labels\n",
            "3298\n",
            "img/52670.png\n",
            "\n",
            "0: 448x640 1 person, 20.8ms\n",
            "Speed: 2.9ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3298\u001b[0m\n",
            "1 label saved to runs/detect/train3298/labels\n",
            "3299\n",
            "img/64902.png\n",
            "\n",
            "0: 224x640 (no detections), 18.7ms\n",
            "Speed: 1.7ms preprocess, 18.7ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3299\u001b[0m\n",
            "0 label saved to runs/detect/train3299/labels\n",
            "3300\n",
            "img/20654.png\n",
            "\n",
            "0: 448x640 7 persons, 19.2ms\n",
            "Speed: 3.0ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3300\u001b[0m\n",
            "1 label saved to runs/detect/train3300/labels\n",
            "3301\n",
            "img/70826.png\n",
            "\n",
            "0: 448x640 1 airplane, 21.5ms\n",
            "Speed: 2.9ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3301\u001b[0m\n",
            "1 label saved to runs/detect/train3301/labels\n",
            "3302\n",
            "img/12983.png\n",
            "\n",
            "0: 448x640 1 person, 17.2ms\n",
            "Speed: 3.1ms preprocess, 17.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3302\u001b[0m\n",
            "1 label saved to runs/detect/train3302/labels\n",
            "3303\n",
            "img/75123.png\n",
            "\n",
            "0: 448x640 7 persons, 17.5ms\n",
            "Speed: 2.7ms preprocess, 17.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3303\u001b[0m\n",
            "1 label saved to runs/detect/train3303/labels\n",
            "3304\n",
            "img/45108.png\n",
            "\n",
            "0: 448x640 1 person, 17.0ms\n",
            "Speed: 2.6ms preprocess, 17.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3304\u001b[0m\n",
            "1 label saved to runs/detect/train3304/labels\n",
            "3305\n",
            "img/40561.png\n",
            "\n",
            "0: 448x640 1 person, 19.7ms\n",
            "Speed: 3.4ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3305\u001b[0m\n",
            "1 label saved to runs/detect/train3305/labels\n",
            "3306\n",
            "img/24095.png\n",
            "\n",
            "0: 640x640 1 person, 21.1ms\n",
            "Speed: 4.2ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3306\u001b[0m\n",
            "1 label saved to runs/detect/train3306/labels\n",
            "3307\n",
            "img/78693.png\n",
            "\n",
            "0: 448x640 5 persons, 1 oven, 24.6ms\n",
            "Speed: 2.8ms preprocess, 24.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3307\u001b[0m\n",
            "1 label saved to runs/detect/train3307/labels\n",
            "3308\n",
            "img/24769.png\n",
            "\n",
            "0: 448x640 1 person, 19.5ms\n",
            "Speed: 4.1ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3308\u001b[0m\n",
            "1 label saved to runs/detect/train3308/labels\n",
            "3309\n",
            "img/92168.png\n",
            "\n",
            "0: 448x640 4 persons, 18.9ms\n",
            "Speed: 3.2ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3309\u001b[0m\n",
            "1 label saved to runs/detect/train3309/labels\n",
            "3310\n",
            "img/74839.png\n",
            "\n",
            "0: 640x480 1 person, 1 tie, 18.7ms\n",
            "Speed: 4.1ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3310\u001b[0m\n",
            "1 label saved to runs/detect/train3310/labels\n",
            "3311\n",
            "img/04132.png\n",
            "\n",
            "0: 448x640 3 persons, 19.5ms\n",
            "Speed: 3.2ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3311\u001b[0m\n",
            "1 label saved to runs/detect/train3311/labels\n",
            "3312\n",
            "img/48173.png\n",
            "\n",
            "0: 448x640 6 persons, 18.7ms\n",
            "Speed: 2.9ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3312\u001b[0m\n",
            "1 label saved to runs/detect/train3312/labels\n",
            "3313\n",
            "img/82947.png\n",
            "\n",
            "0: 448x640 1 person, 1 cup, 17.9ms\n",
            "Speed: 2.8ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3313\u001b[0m\n",
            "1 label saved to runs/detect/train3313/labels\n",
            "3314\n",
            "img/35482.png\n",
            "\n",
            "0: 640x480 1 person, 1 tie, 1 clock, 18.9ms\n",
            "Speed: 2.8ms preprocess, 18.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3314\u001b[0m\n",
            "1 label saved to runs/detect/train3314/labels\n",
            "3315\n",
            "img/72354.png\n",
            "\n",
            "0: 448x640 1 person, 1 chair, 1 book, 17.9ms\n",
            "Speed: 3.0ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3315\u001b[0m\n",
            "1 label saved to runs/detect/train3315/labels\n",
            "3316\n",
            "img/35216.png\n",
            "\n",
            "0: 416x640 1 bed, 19.9ms\n",
            "Speed: 2.6ms preprocess, 19.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3316\u001b[0m\n",
            "1 label saved to runs/detect/train3316/labels\n",
            "3317\n",
            "img/78139.png\n",
            "\n",
            "0: 448x640 (no detections), 19.8ms\n",
            "Speed: 3.2ms preprocess, 19.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3317\u001b[0m\n",
            "0 label saved to runs/detect/train3317/labels\n",
            "3318\n",
            "img/71954.png\n",
            "\n",
            "0: 640x320 5 persons, 18.7ms\n",
            "Speed: 2.3ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3318\u001b[0m\n",
            "1 label saved to runs/detect/train3318/labels\n",
            "3319\n",
            "img/56928.png\n",
            "\n",
            "0: 640x448 1 person, 27.0ms\n",
            "Speed: 3.7ms preprocess, 27.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3319\u001b[0m\n",
            "1 label saved to runs/detect/train3319/labels\n",
            "3320\n",
            "img/40987.png\n",
            "\n",
            "0: 640x480 1 person, 1 dog, 19.1ms\n",
            "Speed: 3.0ms preprocess, 19.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3320\u001b[0m\n",
            "1 label saved to runs/detect/train3320/labels\n",
            "3321\n",
            "img/97026.png\n",
            "\n",
            "0: 448x640 1 teddy bear, 19.8ms\n",
            "Speed: 3.2ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3321\u001b[0m\n",
            "1 label saved to runs/detect/train3321/labels\n",
            "3322\n",
            "img/68472.png\n",
            "\n",
            "0: 448x640 2 persons, 1 sports ball, 20.3ms\n",
            "Speed: 3.0ms preprocess, 20.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3322\u001b[0m\n",
            "1 label saved to runs/detect/train3322/labels\n",
            "3323\n",
            "img/75036.png\n",
            "\n",
            "0: 640x448 1 person, 17.9ms\n",
            "Speed: 2.9ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3323\u001b[0m\n",
            "1 label saved to runs/detect/train3323/labels\n",
            "3324\n",
            "img/84179.png\n",
            "\n",
            "0: 640x384 2 persons, 20.1ms\n",
            "Speed: 3.0ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3324\u001b[0m\n",
            "1 label saved to runs/detect/train3324/labels\n",
            "3325\n",
            "img/14783.png\n",
            "\n",
            "0: 640x416 1 person, 20.2ms\n",
            "Speed: 2.8ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3325\u001b[0m\n",
            "1 label saved to runs/detect/train3325/labels\n",
            "3326\n",
            "img/35860.png\n",
            "\n",
            "0: 640x352 3 persons, 1 car, 1 dog, 18.4ms\n",
            "Speed: 2.7ms preprocess, 18.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3326\u001b[0m\n",
            "1 label saved to runs/detect/train3326/labels\n",
            "3327\n",
            "img/02876.png\n",
            "\n",
            "0: 640x448 1 person, 19.1ms\n",
            "Speed: 2.5ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3327\u001b[0m\n",
            "1 label saved to runs/detect/train3327/labels\n",
            "3328\n",
            "img/80764.png\n",
            "\n",
            "0: 448x640 (no detections), 19.6ms\n",
            "Speed: 2.7ms preprocess, 19.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3328\u001b[0m\n",
            "0 label saved to runs/detect/train3328/labels\n",
            "3329\n",
            "img/72146.png\n",
            "\n",
            "0: 448x640 3 persons, 19.0ms\n",
            "Speed: 3.1ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3329\u001b[0m\n",
            "1 label saved to runs/detect/train3329/labels\n",
            "3330\n",
            "img/21059.png\n",
            "\n",
            "0: 640x640 3 persons, 1 car, 1 giraffe, 24.8ms\n",
            "Speed: 3.7ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3330\u001b[0m\n",
            "1 label saved to runs/detect/train3330/labels\n",
            "3331\n",
            "img/49583.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bowl, 2 oranges, 1 potted plant, 1 dining table, 1 microwave, 1 oven, 19.4ms\n",
            "Speed: 2.8ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3331\u001b[0m\n",
            "1 label saved to runs/detect/train3331/labels\n",
            "3332\n",
            "img/21706.png\n",
            "\n",
            "0: 448x640 1 person, 18.9ms\n",
            "Speed: 3.3ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3332\u001b[0m\n",
            "1 label saved to runs/detect/train3332/labels\n",
            "3333\n",
            "img/50278.png\n",
            "\n",
            "0: 448x640 2 persons, 20.4ms\n",
            "Speed: 2.9ms preprocess, 20.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3333\u001b[0m\n",
            "1 label saved to runs/detect/train3333/labels\n",
            "3334\n",
            "img/25467.png\n",
            "\n",
            "0: 448x640 2 persons, 20.0ms\n",
            "Speed: 2.8ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3334\u001b[0m\n",
            "1 label saved to runs/detect/train3334/labels\n",
            "3335\n",
            "img/96435.png\n",
            "\n",
            "0: 640x448 1 person, 22.2ms\n",
            "Speed: 2.6ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3335\u001b[0m\n",
            "1 label saved to runs/detect/train3335/labels\n",
            "3336\n",
            "img/09756.png\n",
            "\n",
            "0: 512x640 1 person, 21.2ms\n",
            "Speed: 3.9ms preprocess, 21.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3336\u001b[0m\n",
            "1 label saved to runs/detect/train3336/labels\n",
            "3337\n",
            "img/41502.png\n",
            "\n",
            "0: 640x448 2 persons, 21.7ms\n",
            "Speed: 2.5ms preprocess, 21.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3337\u001b[0m\n",
            "1 label saved to runs/detect/train3337/labels\n",
            "3338\n",
            "img/10385.png\n",
            "\n",
            "0: 640x416 1 vase, 20.8ms\n",
            "Speed: 4.3ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3338\u001b[0m\n",
            "1 label saved to runs/detect/train3338/labels\n",
            "3339\n",
            "img/10329.png\n",
            "\n",
            "0: 448x640 1 person, 1 horse, 19.1ms\n",
            "Speed: 2.7ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3339\u001b[0m\n",
            "1 label saved to runs/detect/train3339/labels\n",
            "3340\n",
            "img/21650.png\n",
            "\n",
            "0: 640x288 2 persons, 1 car, 1 motorcycle, 18.9ms\n",
            "Speed: 2.2ms preprocess, 18.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 288)\n",
            "Results saved to \u001b[1mruns/detect/train3340\u001b[0m\n",
            "1 label saved to runs/detect/train3340/labels\n",
            "3341\n",
            "img/30251.png\n",
            "\n",
            "0: 448x640 3 persons, 20.1ms\n",
            "Speed: 2.8ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3341\u001b[0m\n",
            "1 label saved to runs/detect/train3341/labels\n",
            "3342\n",
            "img/62305.png\n",
            "\n",
            "0: 448x640 1 person, 18.9ms\n",
            "Speed: 3.0ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3342\u001b[0m\n",
            "1 label saved to runs/detect/train3342/labels\n",
            "3343\n",
            "img/31280.png\n",
            "\n",
            "0: 448x640 2 persons, 1 cell phone, 20.3ms\n",
            "Speed: 3.6ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3343\u001b[0m\n",
            "1 label saved to runs/detect/train3343/labels\n",
            "3344\n",
            "img/37096.png\n",
            "\n",
            "0: 448x640 4 persons, 2 ties, 19.5ms\n",
            "Speed: 3.9ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3344\u001b[0m\n",
            "1 label saved to runs/detect/train3344/labels\n",
            "3345\n",
            "img/47308.png\n",
            "\n",
            "0: 448x640 6 persons, 1 dog, 20.9ms\n",
            "Speed: 3.0ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3345\u001b[0m\n",
            "1 label saved to runs/detect/train3345/labels\n",
            "3346\n",
            "img/48291.png\n",
            "\n",
            "0: 448x640 2 persons, 1 tie, 18.7ms\n",
            "Speed: 2.7ms preprocess, 18.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3346\u001b[0m\n",
            "1 label saved to runs/detect/train3346/labels\n",
            "3347\n",
            "img/06319.png\n",
            "\n",
            "0: 640x512 1 person, 1 tie, 21.0ms\n",
            "Speed: 3.0ms preprocess, 21.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3347\u001b[0m\n",
            "1 label saved to runs/detect/train3347/labels\n",
            "3348\n",
            "img/56910.png\n",
            "\n",
            "0: 480x640 2 persons, 1 parking meter, 19.0ms\n",
            "Speed: 4.0ms preprocess, 19.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3348\u001b[0m\n",
            "1 label saved to runs/detect/train3348/labels\n",
            "3349\n",
            "img/98517.png\n",
            "\n",
            "0: 640x576 6 persons, 2 horses, 1 sheep, 19.1ms\n",
            "Speed: 6.6ms preprocess, 19.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3349\u001b[0m\n",
            "1 label saved to runs/detect/train3349/labels\n",
            "3350\n",
            "img/50246.png\n",
            "\n",
            "0: 640x480 2 persons, 20.6ms\n",
            "Speed: 3.3ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3350\u001b[0m\n",
            "1 label saved to runs/detect/train3350/labels\n",
            "3351\n",
            "img/25780.png\n",
            "\n",
            "0: 640x576 4 persons, 1 horse, 2 sheeps, 18.5ms\n",
            "Speed: 3.9ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3351\u001b[0m\n",
            "1 label saved to runs/detect/train3351/labels\n",
            "3352\n",
            "img/42078.png\n",
            "\n",
            "0: 640x480 1 tie, 21.7ms\n",
            "Speed: 2.9ms preprocess, 21.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3352\u001b[0m\n",
            "1 label saved to runs/detect/train3352/labels\n",
            "3353\n",
            "img/89621.png\n",
            "\n",
            "0: 448x640 1 person, 1 car, 18.6ms\n",
            "Speed: 3.4ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3353\u001b[0m\n",
            "1 label saved to runs/detect/train3353/labels\n",
            "3354\n",
            "img/27156.png\n",
            "\n",
            "0: 640x576 5 persons, 2 horses, 2 sheeps, 19.4ms\n",
            "Speed: 3.9ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3354\u001b[0m\n",
            "1 label saved to runs/detect/train3354/labels\n",
            "3355\n",
            "img/63910.png\n",
            "\n",
            "0: 576x640 3 persons, 1 bed, 21.8ms\n",
            "Speed: 4.1ms preprocess, 21.8ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3355\u001b[0m\n",
            "1 label saved to runs/detect/train3355/labels\n",
            "3356\n",
            "img/40529.png\n",
            "\n",
            "0: 448x640 1 person, 1 cell phone, 20.4ms\n",
            "Speed: 3.4ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3356\u001b[0m\n",
            "1 label saved to runs/detect/train3356/labels\n",
            "3357\n",
            "img/50732.png\n",
            "\n",
            "0: 640x448 4 persons, 1 dog, 1 sports ball, 19.2ms\n",
            "Speed: 3.3ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3357\u001b[0m\n",
            "1 label saved to runs/detect/train3357/labels\n",
            "3358\n",
            "img/39540.png\n",
            "\n",
            "0: 480x640 4 persons, 2 ties, 5 surfboards, 19.1ms\n",
            "Speed: 3.5ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3358\u001b[0m\n",
            "1 label saved to runs/detect/train3358/labels\n",
            "3359\n",
            "img/24607.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 20.7ms\n",
            "Speed: 3.1ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3359\u001b[0m\n",
            "1 label saved to runs/detect/train3359/labels\n",
            "3360\n",
            "img/46193.png\n",
            "\n",
            "0: 640x448 1 person, 1 tv, 24.2ms\n",
            "Speed: 2.7ms preprocess, 24.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3360\u001b[0m\n",
            "1 label saved to runs/detect/train3360/labels\n",
            "3361\n",
            "img/80926.png\n",
            "\n",
            "0: 512x640 1 person, 1 chair, 2 cell phones, 20.1ms\n",
            "Speed: 4.5ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3361\u001b[0m\n",
            "1 label saved to runs/detect/train3361/labels\n",
            "3362\n",
            "img/59142.png\n",
            "\n",
            "0: 448x640 2 persons, 1 tie, 20.0ms\n",
            "Speed: 3.3ms preprocess, 20.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3362\u001b[0m\n",
            "1 label saved to runs/detect/train3362/labels\n",
            "3363\n",
            "img/28436.png\n",
            "\n",
            "0: 448x640 2 persons, 25 donuts, 19.2ms\n",
            "Speed: 3.9ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3363\u001b[0m\n",
            "1 label saved to runs/detect/train3363/labels\n",
            "3364\n",
            "img/87219.png\n",
            "\n",
            "0: 448x640 1 giraffe, 21.2ms\n",
            "Speed: 3.0ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3364\u001b[0m\n",
            "1 label saved to runs/detect/train3364/labels\n",
            "3365\n",
            "img/18594.png\n",
            "\n",
            "0: 640x448 1 person, 1 cell phone, 23.2ms\n",
            "Speed: 2.6ms preprocess, 23.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3365\u001b[0m\n",
            "1 label saved to runs/detect/train3365/labels\n",
            "3366\n",
            "img/94625.png\n",
            "\n",
            "0: 640x416 7 persons, 1 bottle, 1 cell phone, 19.4ms\n",
            "Speed: 2.4ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3366\u001b[0m\n",
            "1 label saved to runs/detect/train3366/labels\n",
            "3367\n",
            "img/87946.png\n",
            "\n",
            "0: 320x640 6 persons, 1 tie, 5 chairs, 19.3ms\n",
            "Speed: 4.0ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3367\u001b[0m\n",
            "1 label saved to runs/detect/train3367/labels\n",
            "3368\n",
            "img/91083.png\n",
            "\n",
            "0: 448x640 3 persons, 2 bottles, 4 spoons, 6 bowls, 2 hot dogs, 1 dining table, 30.7ms\n",
            "Speed: 3.2ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3368\u001b[0m\n",
            "1 label saved to runs/detect/train3368/labels\n",
            "3369\n",
            "img/58136.png\n",
            "\n",
            "0: 448x640 4 persons, 7 chairs, 22.7ms\n",
            "Speed: 4.5ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3369\u001b[0m\n",
            "1 label saved to runs/detect/train3369/labels\n",
            "3370\n",
            "img/83429.png\n",
            "\n",
            "0: 448x640 1 person, 19.7ms\n",
            "Speed: 2.8ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3370\u001b[0m\n",
            "1 label saved to runs/detect/train3370/labels\n",
            "3371\n",
            "img/02943.png\n",
            "\n",
            "0: 384x640 3 persons, 20.3ms\n",
            "Speed: 2.6ms preprocess, 20.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3371\u001b[0m\n",
            "1 label saved to runs/detect/train3371/labels\n",
            "3372\n",
            "img/51726.png\n",
            "\n",
            "0: 640x640 1 person, 18.6ms\n",
            "Speed: 5.0ms preprocess, 18.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3372\u001b[0m\n",
            "1 label saved to runs/detect/train3372/labels\n",
            "3373\n",
            "img/14087.png\n",
            "\n",
            "0: 640x448 1 person, 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3373\u001b[0m\n",
            "1 label saved to runs/detect/train3373/labels\n",
            "3374\n",
            "img/87309.png\n",
            "\n",
            "0: 640x448 1 person, 18.0ms\n",
            "Speed: 3.1ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3374\u001b[0m\n",
            "1 label saved to runs/detect/train3374/labels\n",
            "3375\n",
            "img/76429.png\n",
            "\n",
            "0: 640x448 3 persons, 1 tie, 18.8ms\n",
            "Speed: 2.6ms preprocess, 18.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3375\u001b[0m\n",
            "1 label saved to runs/detect/train3375/labels\n",
            "3376\n",
            "img/82719.png\n",
            "\n",
            "0: 512x640 5 persons, 1 bicycle, 1 motorcycle, 20.9ms\n",
            "Speed: 3.2ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3376\u001b[0m\n",
            "1 label saved to runs/detect/train3376/labels\n",
            "3377\n",
            "img/48175.png\n",
            "\n",
            "0: 448x640 1 person, 22.6ms\n",
            "Speed: 3.1ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3377\u001b[0m\n",
            "1 label saved to runs/detect/train3377/labels\n",
            "3378\n",
            "img/70285.png\n",
            "\n",
            "0: 640x544 1 person, 18.9ms\n",
            "Speed: 4.5ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3378\u001b[0m\n",
            "1 label saved to runs/detect/train3378/labels\n",
            "3379\n",
            "img/84370.png\n",
            "\n",
            "0: 448x640 1 person, 20.7ms\n",
            "Speed: 3.1ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3379\u001b[0m\n",
            "1 label saved to runs/detect/train3379/labels\n",
            "3380\n",
            "img/25340.png\n",
            "\n",
            "0: 416x640 (no detections), 21.4ms\n",
            "Speed: 4.5ms preprocess, 21.4ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3380\u001b[0m\n",
            "0 label saved to runs/detect/train3380/labels\n",
            "3381\n",
            "img/04938.png\n",
            "\n",
            "0: 448x640 3 persons, 21.5ms\n",
            "Speed: 3.4ms preprocess, 21.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3381\u001b[0m\n",
            "1 label saved to runs/detect/train3381/labels\n",
            "3382\n",
            "img/27836.png\n",
            "\n",
            "0: 480x640 6 persons, 6 ties, 18.9ms\n",
            "Speed: 3.6ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3382\u001b[0m\n",
            "1 label saved to runs/detect/train3382/labels\n",
            "3383\n",
            "img/79652.png\n",
            "\n",
            "0: 448x640 7 persons, 3 handbags, 1 tie, 20.6ms\n",
            "Speed: 3.3ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3383\u001b[0m\n",
            "1 label saved to runs/detect/train3383/labels\n",
            "3384\n",
            "img/36175.png\n",
            "\n",
            "0: 448x640 1 person, 1 bottle, 1 microwave, 4 ovens, 21.3ms\n",
            "Speed: 3.5ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3384\u001b[0m\n",
            "1 label saved to runs/detect/train3384/labels\n",
            "3385\n",
            "img/82943.png\n",
            "\n",
            "0: 384x640 1 person, 1 cat, 21.4ms\n",
            "Speed: 2.9ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3385\u001b[0m\n",
            "1 label saved to runs/detect/train3385/labels\n",
            "3386\n",
            "img/74630.png\n",
            "\n",
            "0: 640x480 3 persons, 20.7ms\n",
            "Speed: 4.3ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3386\u001b[0m\n",
            "1 label saved to runs/detect/train3386/labels\n",
            "3387\n",
            "img/42897.png\n",
            "\n",
            "0: 448x640 3 persons, 20.2ms\n",
            "Speed: 3.9ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3387\u001b[0m\n",
            "1 label saved to runs/detect/train3387/labels\n",
            "3388\n",
            "img/81764.png\n",
            "\n",
            "0: 640x544 1 person, 19.7ms\n",
            "Speed: 3.9ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3388\u001b[0m\n",
            "1 label saved to runs/detect/train3388/labels\n",
            "3389\n",
            "img/45370.png\n",
            "\n",
            "0: 640x480 2 persons, 1 tie, 1 cell phone, 20.1ms\n",
            "Speed: 4.1ms preprocess, 20.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3389\u001b[0m\n",
            "1 label saved to runs/detect/train3389/labels\n",
            "3390\n",
            "img/67014.png\n",
            "\n",
            "0: 640x384 2 persons, 22.2ms\n",
            "Speed: 2.9ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3390\u001b[0m\n",
            "1 label saved to runs/detect/train3390/labels\n",
            "3391\n",
            "img/51086.png\n",
            "\n",
            "0: 448x640 (no detections), 20.5ms\n",
            "Speed: 2.7ms preprocess, 20.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3391\u001b[0m\n",
            "0 label saved to runs/detect/train3391/labels\n",
            "3392\n",
            "img/73984.png\n",
            "\n",
            "0: 640x224 2 persons, 35.4ms\n",
            "Speed: 1.6ms preprocess, 35.4ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 224)\n",
            "Results saved to \u001b[1mruns/detect/train3392\u001b[0m\n",
            "1 label saved to runs/detect/train3392/labels\n",
            "3393\n",
            "img/35087.png\n",
            "\n",
            "0: 640x448 1 person, 1 hot dog, 32.9ms\n",
            "Speed: 2.9ms preprocess, 32.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3393\u001b[0m\n",
            "1 label saved to runs/detect/train3393/labels\n",
            "3394\n",
            "img/27895.png\n",
            "\n",
            "0: 640x384 1 person, 31.4ms\n",
            "Speed: 3.6ms preprocess, 31.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3394\u001b[0m\n",
            "1 label saved to runs/detect/train3394/labels\n",
            "3395\n",
            "img/16039.png\n",
            "\n",
            "0: 640x480 2 persons, 25.0ms\n",
            "Speed: 3.8ms preprocess, 25.0ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3395\u001b[0m\n",
            "1 label saved to runs/detect/train3395/labels\n",
            "3396\n",
            "img/49863.png\n",
            "\n",
            "0: 512x640 1 person, 1 tie, 24.4ms\n",
            "Speed: 3.7ms preprocess, 24.4ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3396\u001b[0m\n",
            "1 label saved to runs/detect/train3396/labels\n",
            "3397\n",
            "img/50167.png\n",
            "\n",
            "0: 448x640 1 person, 24.0ms\n",
            "Speed: 7.5ms preprocess, 24.0ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3397\u001b[0m\n",
            "1 label saved to runs/detect/train3397/labels\n",
            "3398\n",
            "img/16048.png\n",
            "\n",
            "0: 448x640 9 persons, 1 bottle, 5 cups, 2 chairs, 2 dining tables, 29.6ms\n",
            "Speed: 3.7ms preprocess, 29.6ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3398\u001b[0m\n",
            "1 label saved to runs/detect/train3398/labels\n",
            "3399\n",
            "img/41962.png\n",
            "\n",
            "0: 640x512 1 person, 1 car, 1 truck, 25.9ms\n",
            "Speed: 3.6ms preprocess, 25.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3399\u001b[0m\n",
            "1 label saved to runs/detect/train3399/labels\n",
            "3400\n",
            "img/85093.png\n",
            "\n",
            "0: 640x480 2 persons, 24.7ms\n",
            "Speed: 3.0ms preprocess, 24.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3400\u001b[0m\n",
            "1 label saved to runs/detect/train3400/labels\n",
            "3401\n",
            "img/31209.png\n",
            "\n",
            "0: 448x640 2 persons, 1 tie, 31.6ms\n",
            "Speed: 3.3ms preprocess, 31.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3401\u001b[0m\n",
            "1 label saved to runs/detect/train3401/labels\n",
            "3402\n",
            "img/37902.png\n",
            "\n",
            "0: 416x640 (no detections), 29.6ms\n",
            "Speed: 6.9ms preprocess, 29.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3402\u001b[0m\n",
            "0 label saved to runs/detect/train3402/labels\n",
            "3403\n",
            "img/20795.png\n",
            "\n",
            "0: 384x640 1 person, 1 tie, 25.1ms\n",
            "Speed: 2.8ms preprocess, 25.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3403\u001b[0m\n",
            "1 label saved to runs/detect/train3403/labels\n",
            "3404\n",
            "img/47029.png\n",
            "\n",
            "0: 448x640 6 persons, 2 baseball bats, 30.4ms\n",
            "Speed: 2.7ms preprocess, 30.4ms inference, 5.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3404\u001b[0m\n",
            "1 label saved to runs/detect/train3404/labels\n",
            "3405\n",
            "img/08163.png\n",
            "\n",
            "0: 640x512 1 dog, 26.6ms\n",
            "Speed: 2.8ms preprocess, 26.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3405\u001b[0m\n",
            "1 label saved to runs/detect/train3405/labels\n",
            "3406\n",
            "img/28479.png\n",
            "\n",
            "0: 640x448 1 person, 29.7ms\n",
            "Speed: 3.3ms preprocess, 29.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3406\u001b[0m\n",
            "1 label saved to runs/detect/train3406/labels\n",
            "3407\n",
            "img/39576.png\n",
            "\n",
            "0: 640x448 1 person, 24.3ms\n",
            "Speed: 3.0ms preprocess, 24.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3407\u001b[0m\n",
            "1 label saved to runs/detect/train3407/labels\n",
            "3408\n",
            "img/38654.png\n",
            "\n",
            "0: 640x640 1 person, 1 umbrella, 26.4ms\n",
            "Speed: 4.3ms preprocess, 26.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3408\u001b[0m\n",
            "1 label saved to runs/detect/train3408/labels\n",
            "3409\n",
            "img/05127.png\n",
            "\n",
            "0: 448x640 (no detections), 22.8ms\n",
            "Speed: 3.0ms preprocess, 22.8ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3409\u001b[0m\n",
            "0 label saved to runs/detect/train3409/labels\n",
            "3410\n",
            "img/59602.png\n",
            "\n",
            "0: 448x640 1 person, 24.2ms\n",
            "Speed: 2.7ms preprocess, 24.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3410\u001b[0m\n",
            "1 label saved to runs/detect/train3410/labels\n",
            "3411\n",
            "img/42861.png\n",
            "\n",
            "0: 640x448 1 person, 23.5ms\n",
            "Speed: 3.0ms preprocess, 23.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3411\u001b[0m\n",
            "1 label saved to runs/detect/train3411/labels\n",
            "3412\n",
            "img/93168.png\n",
            "\n",
            "0: 640x448 (no detections), 35.4ms\n",
            "Speed: 3.1ms preprocess, 35.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3412\u001b[0m\n",
            "0 label saved to runs/detect/train3412/labels\n",
            "3413\n",
            "img/89726.png\n",
            "\n",
            "0: 448x640 3 persons, 33.7ms\n",
            "Speed: 3.0ms preprocess, 33.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3413\u001b[0m\n",
            "1 label saved to runs/detect/train3413/labels\n",
            "3414\n",
            "img/51269.png\n",
            "\n",
            "0: 416x640 2 persons, 26.1ms\n",
            "Speed: 3.2ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3414\u001b[0m\n",
            "1 label saved to runs/detect/train3414/labels\n",
            "3415\n",
            "img/70492.png\n",
            "\n",
            "0: 448x640 6 persons, 1 baseball glove, 26.3ms\n",
            "Speed: 2.5ms preprocess, 26.3ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3415\u001b[0m\n",
            "1 label saved to runs/detect/train3415/labels\n",
            "3416\n",
            "img/53096.png\n",
            "\n",
            "0: 640x448 1 person, 29.4ms\n",
            "Speed: 3.4ms preprocess, 29.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3416\u001b[0m\n",
            "1 label saved to runs/detect/train3416/labels\n",
            "3417\n",
            "img/49328.png\n",
            "\n",
            "0: 640x448 3 persons, 30.7ms\n",
            "Speed: 3.2ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3417\u001b[0m\n",
            "1 label saved to runs/detect/train3417/labels\n",
            "3418\n",
            "img/67241.png\n",
            "\n",
            "0: 416x640 1 dog, 29.5ms\n",
            "Speed: 3.2ms preprocess, 29.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3418\u001b[0m\n",
            "1 label saved to runs/detect/train3418/labels\n",
            "3419\n",
            "img/49685.png\n",
            "\n",
            "0: 448x640 1 clock, 25.7ms\n",
            "Speed: 2.9ms preprocess, 25.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3419\u001b[0m\n",
            "1 label saved to runs/detect/train3419/labels\n",
            "3420\n",
            "img/85902.png\n",
            "\n",
            "0: 448x640 1 person, 1 car, 1 backpack, 1 handbag, 30.0ms\n",
            "Speed: 3.0ms preprocess, 30.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3420\u001b[0m\n",
            "1 label saved to runs/detect/train3420/labels\n",
            "3421\n",
            "img/08439.png\n",
            "\n",
            "0: 448x640 1 person, 23.8ms\n",
            "Speed: 2.9ms preprocess, 23.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3421\u001b[0m\n",
            "1 label saved to runs/detect/train3421/labels\n",
            "3422\n",
            "img/78095.png\n",
            "\n",
            "0: 448x640 1 person, 1 bed, 27.2ms\n",
            "Speed: 3.3ms preprocess, 27.2ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3422\u001b[0m\n",
            "1 label saved to runs/detect/train3422/labels\n",
            "3423\n",
            "img/32540.png\n",
            "\n",
            "0: 448x640 5 persons, 1 apple, 25.4ms\n",
            "Speed: 3.2ms preprocess, 25.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3423\u001b[0m\n",
            "1 label saved to runs/detect/train3423/labels\n",
            "3424\n",
            "img/09648.png\n",
            "\n",
            "0: 480x640 1 person, 33.4ms\n",
            "Speed: 3.1ms preprocess, 33.4ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3424\u001b[0m\n",
            "1 label saved to runs/detect/train3424/labels\n",
            "3425\n",
            "img/95267.png\n",
            "\n",
            "0: 448x640 3 persons, 1 bed, 34.5ms\n",
            "Speed: 3.3ms preprocess, 34.5ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3425\u001b[0m\n",
            "1 label saved to runs/detect/train3425/labels\n",
            "3426\n",
            "img/93506.png\n",
            "\n",
            "0: 448x640 1 spoon, 1 bowl, 1 dining table, 26.8ms\n",
            "Speed: 6.9ms preprocess, 26.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3426\u001b[0m\n",
            "1 label saved to runs/detect/train3426/labels\n",
            "3427\n",
            "img/21589.png\n",
            "\n",
            "0: 448x640 3 persons, 20.5ms\n",
            "Speed: 3.3ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3427\u001b[0m\n",
            "1 label saved to runs/detect/train3427/labels\n",
            "3428\n",
            "img/13976.png\n",
            "\n",
            "0: 640x448 1 person, 26.2ms\n",
            "Speed: 2.7ms preprocess, 26.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3428\u001b[0m\n",
            "1 label saved to runs/detect/train3428/labels\n",
            "3429\n",
            "img/58023.png\n",
            "\n",
            "0: 640x480 2 persons, 21.1ms\n",
            "Speed: 3.0ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3429\u001b[0m\n",
            "1 label saved to runs/detect/train3429/labels\n",
            "3430\n",
            "img/01235.png\n",
            "\n",
            "0: 448x640 1 person, 19.9ms\n",
            "Speed: 3.1ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3430\u001b[0m\n",
            "1 label saved to runs/detect/train3430/labels\n",
            "3431\n",
            "img/51293.png\n",
            "\n",
            "0: 416x640 1 person, 18.7ms\n",
            "Speed: 2.8ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3431\u001b[0m\n",
            "1 label saved to runs/detect/train3431/labels\n",
            "3432\n",
            "img/52863.png\n",
            "\n",
            "0: 448x640 2 persons, 18.5ms\n",
            "Speed: 3.0ms preprocess, 18.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3432\u001b[0m\n",
            "1 label saved to runs/detect/train3432/labels\n",
            "3433\n",
            "img/20371.png\n",
            "\n",
            "0: 512x640 1 person, 22.6ms\n",
            "Speed: 8.2ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3433\u001b[0m\n",
            "1 label saved to runs/detect/train3433/labels\n",
            "3434\n",
            "img/96514.png\n",
            "\n",
            "0: 640x640 (no detections), 20.7ms\n",
            "Speed: 3.4ms preprocess, 20.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3434\u001b[0m\n",
            "0 label saved to runs/detect/train3434/labels\n",
            "3435\n",
            "img/37945.png\n",
            "\n",
            "0: 448x640 4 persons, 2 traffic lights, 1 cell phone, 20.6ms\n",
            "Speed: 3.8ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3435\u001b[0m\n",
            "1 label saved to runs/detect/train3435/labels\n",
            "3436\n",
            "img/06795.png\n",
            "\n",
            "0: 640x640 1 cat, 18.4ms\n",
            "Speed: 5.2ms preprocess, 18.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3436\u001b[0m\n",
            "1 label saved to runs/detect/train3436/labels\n",
            "3437\n",
            "img/87403.png\n",
            "\n",
            "0: 448x640 1 person, 19.5ms\n",
            "Speed: 3.4ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3437\u001b[0m\n",
            "1 label saved to runs/detect/train3437/labels\n",
            "3438\n",
            "img/02631.png\n",
            "\n",
            "0: 448x640 1 person, 17.6ms\n",
            "Speed: 3.0ms preprocess, 17.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3438\u001b[0m\n",
            "1 label saved to runs/detect/train3438/labels\n",
            "3439\n",
            "img/64593.png\n",
            "\n",
            "0: 448x640 2 persons, 18.1ms\n",
            "Speed: 4.2ms preprocess, 18.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3439\u001b[0m\n",
            "1 label saved to runs/detect/train3439/labels\n",
            "3440\n",
            "img/36821.png\n",
            "\n",
            "0: 416x640 1 person, 21.2ms\n",
            "Speed: 3.5ms preprocess, 21.2ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3440\u001b[0m\n",
            "1 label saved to runs/detect/train3440/labels\n",
            "3441\n",
            "img/31487.png\n",
            "\n",
            "0: 448x640 1 person, 20.3ms\n",
            "Speed: 3.1ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3441\u001b[0m\n",
            "1 label saved to runs/detect/train3441/labels\n",
            "3442\n",
            "img/97250.png\n",
            "\n",
            "0: 448x640 1 person, 20.1ms\n",
            "Speed: 2.9ms preprocess, 20.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3442\u001b[0m\n",
            "1 label saved to runs/detect/train3442/labels\n",
            "3443\n",
            "img/87392.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 20.8ms\n",
            "Speed: 3.4ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3443\u001b[0m\n",
            "1 label saved to runs/detect/train3443/labels\n",
            "3444\n",
            "img/21780.png\n",
            "\n",
            "0: 512x640 1 person, 20.9ms\n",
            "Speed: 3.9ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3444\u001b[0m\n",
            "1 label saved to runs/detect/train3444/labels\n",
            "3445\n",
            "img/03861.png\n",
            "\n",
            "0: 448x640 1 person, 26.9ms\n",
            "Speed: 2.9ms preprocess, 26.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3445\u001b[0m\n",
            "1 label saved to runs/detect/train3445/labels\n",
            "3446\n",
            "img/50163.png\n",
            "\n",
            "0: 448x640 1 person, 8 sheeps, 19.4ms\n",
            "Speed: 2.9ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3446\u001b[0m\n",
            "1 label saved to runs/detect/train3446/labels\n",
            "3447\n",
            "img/14067.png\n",
            "\n",
            "0: 640x448 1 person, 19.1ms\n",
            "Speed: 3.2ms preprocess, 19.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3447\u001b[0m\n",
            "1 label saved to runs/detect/train3447/labels\n",
            "3448\n",
            "img/10579.png\n",
            "\n",
            "0: 640x448 1 person, 20.6ms\n",
            "Speed: 3.0ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3448\u001b[0m\n",
            "1 label saved to runs/detect/train3448/labels\n",
            "3449\n",
            "img/12807.png\n",
            "\n",
            "0: 448x640 1 cat, 21.2ms\n",
            "Speed: 3.1ms preprocess, 21.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3449\u001b[0m\n",
            "1 label saved to runs/detect/train3449/labels\n",
            "3450\n",
            "img/63850.png\n",
            "\n",
            "0: 640x640 1 cake, 20.7ms\n",
            "Speed: 5.3ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3450\u001b[0m\n",
            "1 label saved to runs/detect/train3450/labels\n",
            "3451\n",
            "img/60374.png\n",
            "\n",
            "0: 640x512 1 dog, 22.5ms\n",
            "Speed: 3.7ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3451\u001b[0m\n",
            "1 label saved to runs/detect/train3451/labels\n",
            "3452\n",
            "img/79605.png\n",
            "\n",
            "0: 448x640 16 persons, 1 car, 1 truck, 1 cow, 20.6ms\n",
            "Speed: 4.0ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3452\u001b[0m\n",
            "1 label saved to runs/detect/train3452/labels\n",
            "3453\n",
            "img/16049.png\n",
            "\n",
            "0: 448x640 1 person, 21.4ms\n",
            "Speed: 3.1ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3453\u001b[0m\n",
            "1 label saved to runs/detect/train3453/labels\n",
            "3454\n",
            "img/28561.png\n",
            "\n",
            "0: 448x640 11 persons, 18.7ms\n",
            "Speed: 3.0ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3454\u001b[0m\n",
            "1 label saved to runs/detect/train3454/labels\n",
            "3455\n",
            "img/08276.png\n",
            "\n",
            "0: 448x640 2 persons, 18.5ms\n",
            "Speed: 2.8ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3455\u001b[0m\n",
            "1 label saved to runs/detect/train3455/labels\n",
            "3456\n",
            "img/71509.png\n",
            "\n",
            "0: 640x448 1 person, 1 horse, 22.0ms\n",
            "Speed: 3.0ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3456\u001b[0m\n",
            "1 label saved to runs/detect/train3456/labels\n",
            "3457\n",
            "img/27153.png\n",
            "\n",
            "0: 640x480 2 persons, 1 cell phone, 27.7ms\n",
            "Speed: 2.9ms preprocess, 27.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3457\u001b[0m\n",
            "1 label saved to runs/detect/train3457/labels\n",
            "3458\n",
            "img/87914.png\n",
            "\n",
            "0: 640x448 3 persons, 1 bicycle, 1 car, 1 tennis racket, 19.7ms\n",
            "Speed: 2.7ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3458\u001b[0m\n",
            "1 label saved to runs/detect/train3458/labels\n",
            "3459\n",
            "img/78260.png\n",
            "\n",
            "0: 544x640 1 person, 19.5ms\n",
            "Speed: 3.2ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3459\u001b[0m\n",
            "1 label saved to runs/detect/train3459/labels\n",
            "3460\n",
            "img/86250.png\n",
            "\n",
            "0: 448x640 2 persons, 19.0ms\n",
            "Speed: 2.7ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3460\u001b[0m\n",
            "1 label saved to runs/detect/train3460/labels\n",
            "3461\n",
            "img/65283.png\n",
            "\n",
            "0: 640x480 1 person, 19.9ms\n",
            "Speed: 2.9ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3461\u001b[0m\n",
            "1 label saved to runs/detect/train3461/labels\n",
            "3462\n",
            "img/49271.png\n",
            "\n",
            "0: 640x480 1 person, 1 backpack, 21.0ms\n",
            "Speed: 3.2ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3462\u001b[0m\n",
            "1 label saved to runs/detect/train3462/labels\n",
            "3463\n",
            "img/69250.png\n",
            "\n",
            "0: 640x480 1 person, 1 tie, 18.7ms\n",
            "Speed: 3.4ms preprocess, 18.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3463\u001b[0m\n",
            "1 label saved to runs/detect/train3463/labels\n",
            "3464\n",
            "img/51864.png\n",
            "\n",
            "0: 640x448 4 persons, 23.2ms\n",
            "Speed: 3.3ms preprocess, 23.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3464\u001b[0m\n",
            "1 label saved to runs/detect/train3464/labels\n",
            "3465\n",
            "img/41852.png\n",
            "\n",
            "0: 448x640 13 persons, 24.2ms\n",
            "Speed: 3.5ms preprocess, 24.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3465\u001b[0m\n",
            "1 label saved to runs/detect/train3465/labels\n",
            "3466\n",
            "img/56187.png\n",
            "\n",
            "0: 640x448 1 person, 21.3ms\n",
            "Speed: 2.9ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3466\u001b[0m\n",
            "1 label saved to runs/detect/train3466/labels\n",
            "3467\n",
            "img/90286.png\n",
            "\n",
            "0: 448x640 1 person, 20.6ms\n",
            "Speed: 3.8ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3467\u001b[0m\n",
            "1 label saved to runs/detect/train3467/labels\n",
            "3468\n",
            "img/70841.png\n",
            "\n",
            "0: 448x640 1 person, 19.4ms\n",
            "Speed: 3.1ms preprocess, 19.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3468\u001b[0m\n",
            "1 label saved to runs/detect/train3468/labels\n",
            "3469\n",
            "img/57923.png\n",
            "\n",
            "0: 416x640 6 persons, 25.5ms\n",
            "Speed: 2.8ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3469\u001b[0m\n",
            "1 label saved to runs/detect/train3469/labels\n",
            "3470\n",
            "img/39624.png\n",
            "\n",
            "0: 448x640 3 persons, 19.4ms\n",
            "Speed: 3.2ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3470\u001b[0m\n",
            "1 label saved to runs/detect/train3470/labels\n",
            "3471\n",
            "img/87634.png\n",
            "\n",
            "0: 448x640 1 person, 19.8ms\n",
            "Speed: 3.1ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3471\u001b[0m\n",
            "1 label saved to runs/detect/train3471/labels\n",
            "3472\n",
            "img/56721.png\n",
            "\n",
            "0: 448x640 1 person, 18.3ms\n",
            "Speed: 4.5ms preprocess, 18.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3472\u001b[0m\n",
            "1 label saved to runs/detect/train3472/labels\n",
            "3473\n",
            "img/07481.png\n",
            "\n",
            "0: 448x640 1 sheep, 3 cows, 17.3ms\n",
            "Speed: 2.8ms preprocess, 17.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3473\u001b[0m\n",
            "1 label saved to runs/detect/train3473/labels\n",
            "3474\n",
            "img/51249.png\n",
            "\n",
            "0: 448x640 1 person, 18.4ms\n",
            "Speed: 2.6ms preprocess, 18.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3474\u001b[0m\n",
            "1 label saved to runs/detect/train3474/labels\n",
            "3475\n",
            "img/21048.png\n",
            "\n",
            "0: 640x544 1 truck, 2 cows, 20.8ms\n",
            "Speed: 4.4ms preprocess, 20.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3475\u001b[0m\n",
            "1 label saved to runs/detect/train3475/labels\n",
            "3476\n",
            "img/83571.png\n",
            "\n",
            "0: 448x640 1 person, 1 cell phone, 19.9ms\n",
            "Speed: 3.6ms preprocess, 19.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3476\u001b[0m\n",
            "1 label saved to runs/detect/train3476/labels\n",
            "3477\n",
            "img/09268.png\n",
            "\n",
            "0: 448x640 1 person, 2 dogs, 17.7ms\n",
            "Speed: 2.7ms preprocess, 17.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3477\u001b[0m\n",
            "1 label saved to runs/detect/train3477/labels\n",
            "3478\n",
            "img/74158.png\n",
            "\n",
            "0: 640x448 4 bottles, 1 vase, 20.5ms\n",
            "Speed: 2.8ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3478\u001b[0m\n",
            "1 label saved to runs/detect/train3478/labels\n",
            "3479\n",
            "img/05784.png\n",
            "\n",
            "0: 448x640 1 person, 20.9ms\n",
            "Speed: 3.6ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3479\u001b[0m\n",
            "1 label saved to runs/detect/train3479/labels\n",
            "3480\n",
            "img/59708.png\n",
            "\n",
            "0: 640x480 1 person, 1 chair, 18.9ms\n",
            "Speed: 3.6ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3480\u001b[0m\n",
            "1 label saved to runs/detect/train3480/labels\n",
            "3481\n",
            "img/50834.png\n",
            "\n",
            "0: 480x640 3 persons, 1 tie, 19.3ms\n",
            "Speed: 2.7ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3481\u001b[0m\n",
            "1 label saved to runs/detect/train3481/labels\n",
            "3482\n",
            "img/07269.png\n",
            "\n",
            "0: 480x640 1 person, 1 bottle, 1 bowl, 1 banana, 1 chair, 19.1ms\n",
            "Speed: 3.4ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3482\u001b[0m\n",
            "1 label saved to runs/detect/train3482/labels\n",
            "3483\n",
            "img/97845.png\n",
            "\n",
            "0: 640x448 (no detections), 19.5ms\n",
            "Speed: 2.8ms preprocess, 19.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3483\u001b[0m\n",
            "0 label saved to runs/detect/train3483/labels\n",
            "3484\n",
            "img/45219.png\n",
            "\n",
            "0: 640x640 1 person, 19.9ms\n",
            "Speed: 4.0ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3484\u001b[0m\n",
            "1 label saved to runs/detect/train3484/labels\n",
            "3485\n",
            "img/67058.png\n",
            "\n",
            "0: 448x640 1 person, 20.0ms\n",
            "Speed: 3.2ms preprocess, 20.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3485\u001b[0m\n",
            "1 label saved to runs/detect/train3485/labels\n",
            "3486\n",
            "img/47096.png\n",
            "\n",
            "0: 448x640 2 persons, 19.3ms\n",
            "Speed: 3.1ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3486\u001b[0m\n",
            "1 label saved to runs/detect/train3486/labels\n",
            "3487\n",
            "img/09642.png\n",
            "\n",
            "0: 640x512 1 person, 22.0ms\n",
            "Speed: 4.0ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3487\u001b[0m\n",
            "1 label saved to runs/detect/train3487/labels\n",
            "3488\n",
            "img/45603.png\n",
            "\n",
            "0: 480x640 2 persons, 21.5ms\n",
            "Speed: 3.9ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3488\u001b[0m\n",
            "1 label saved to runs/detect/train3488/labels\n",
            "3489\n",
            "img/53901.png\n",
            "\n",
            "0: 640x480 2 persons, 20.0ms\n",
            "Speed: 4.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3489\u001b[0m\n",
            "1 label saved to runs/detect/train3489/labels\n",
            "3490\n",
            "img/94501.png\n",
            "\n",
            "0: 448x640 5 persons, 3 chairs, 22.9ms\n",
            "Speed: 3.2ms preprocess, 22.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3490\u001b[0m\n",
            "1 label saved to runs/detect/train3490/labels\n",
            "3491\n",
            "img/62934.png\n",
            "\n",
            "0: 640x640 1 person, 22.3ms\n",
            "Speed: 4.0ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3491\u001b[0m\n",
            "1 label saved to runs/detect/train3491/labels\n",
            "3492\n",
            "img/37129.png\n",
            "\n",
            "0: 416x640 (no detections), 20.1ms\n",
            "Speed: 2.6ms preprocess, 20.1ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3492\u001b[0m\n",
            "0 label saved to runs/detect/train3492/labels\n",
            "3493\n",
            "img/63087.png\n",
            "\n",
            "0: 640x512 2 persons, 3 horses, 1 tie, 19.9ms\n",
            "Speed: 3.5ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3493\u001b[0m\n",
            "1 label saved to runs/detect/train3493/labels\n",
            "3494\n",
            "img/52487.png\n",
            "\n",
            "0: 448x640 2 persons, 3 bottles, 2 cups, 2 sinks, 23.1ms\n",
            "Speed: 3.1ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3494\u001b[0m\n",
            "1 label saved to runs/detect/train3494/labels\n",
            "3495\n",
            "img/47203.png\n",
            "\n",
            "0: 448x640 1 person, 1 umbrella, 19.4ms\n",
            "Speed: 3.6ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3495\u001b[0m\n",
            "1 label saved to runs/detect/train3495/labels\n",
            "3496\n",
            "img/78406.png\n",
            "\n",
            "0: 640x448 2 persons, 1 traffic light, 19.5ms\n",
            "Speed: 2.9ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3496\u001b[0m\n",
            "1 label saved to runs/detect/train3496/labels\n",
            "3497\n",
            "img/57619.png\n",
            "\n",
            "0: 640x448 3 persons, 1 wine glass, 1 donut, 20.2ms\n",
            "Speed: 3.2ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3497\u001b[0m\n",
            "1 label saved to runs/detect/train3497/labels\n",
            "3498\n",
            "img/76152.png\n",
            "\n",
            "0: 640x320 3 persons, 1 car, 20.0ms\n",
            "Speed: 3.1ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3498\u001b[0m\n",
            "1 label saved to runs/detect/train3498/labels\n",
            "3499\n",
            "img/52386.png\n",
            "\n",
            "0: 640x384 6 persons, 2 cups, 2 chairs, 1 dining table, 20.3ms\n",
            "Speed: 4.2ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3499\u001b[0m\n",
            "1 label saved to runs/detect/train3499/labels\n",
            "3500\n",
            "img/42806.png\n",
            "\n",
            "0: 576x640 3 dogs, 2 cows, 20.3ms\n",
            "Speed: 3.7ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3500\u001b[0m\n",
            "1 label saved to runs/detect/train3500/labels\n",
            "3501\n",
            "img/90413.png\n",
            "\n",
            "0: 448x640 1 couch, 1 potted plant, 1 sink, 19.5ms\n",
            "Speed: 3.2ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3501\u001b[0m\n",
            "1 label saved to runs/detect/train3501/labels\n",
            "3502\n",
            "img/56921.png\n",
            "\n",
            "0: 640x448 1 person, 1 donut, 20.7ms\n",
            "Speed: 2.7ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3502\u001b[0m\n",
            "1 label saved to runs/detect/train3502/labels\n",
            "3503\n",
            "img/32168.png\n",
            "\n",
            "0: 640x448 3 persons, 1 handbag, 17.4ms\n",
            "Speed: 3.1ms preprocess, 17.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3503\u001b[0m\n",
            "1 label saved to runs/detect/train3503/labels\n",
            "3504\n",
            "img/74563.png\n",
            "\n",
            "0: 448x640 2 persons, 19.5ms\n",
            "Speed: 3.7ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3504\u001b[0m\n",
            "1 label saved to runs/detect/train3504/labels\n",
            "3505\n",
            "img/31024.png\n",
            "\n",
            "0: 448x640 1 person, 1 car, 1 truck, 19.3ms\n",
            "Speed: 2.9ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3505\u001b[0m\n",
            "1 label saved to runs/detect/train3505/labels\n",
            "3506\n",
            "img/19026.png\n",
            "\n",
            "0: 640x448 1 person, 22.0ms\n",
            "Speed: 2.7ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3506\u001b[0m\n",
            "1 label saved to runs/detect/train3506/labels\n",
            "3507\n",
            "img/09374.png\n",
            "\n",
            "0: 448x640 1 person, 1 car, 1 truck, 19.3ms\n",
            "Speed: 2.9ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3507\u001b[0m\n",
            "1 label saved to runs/detect/train3507/labels\n",
            "3508\n",
            "img/32947.png\n",
            "\n",
            "0: 448x640 3 persons, 18.5ms\n",
            "Speed: 3.2ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3508\u001b[0m\n",
            "1 label saved to runs/detect/train3508/labels\n",
            "3509\n",
            "img/56987.png\n",
            "\n",
            "0: 448x640 1 person, 20.3ms\n",
            "Speed: 3.2ms preprocess, 20.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3509\u001b[0m\n",
            "1 label saved to runs/detect/train3509/labels\n",
            "3510\n",
            "img/82765.png\n",
            "\n",
            "0: 448x640 2 persons, 20.7ms\n",
            "Speed: 3.6ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3510\u001b[0m\n",
            "1 label saved to runs/detect/train3510/labels\n",
            "3511\n",
            "img/08375.png\n",
            "\n",
            "0: 640x448 2 persons, 19.6ms\n",
            "Speed: 2.5ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3511\u001b[0m\n",
            "1 label saved to runs/detect/train3511/labels\n",
            "3512\n",
            "img/49682.png\n",
            "\n",
            "0: 640x448 1 person, 20.9ms\n",
            "Speed: 3.0ms preprocess, 20.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3512\u001b[0m\n",
            "1 label saved to runs/detect/train3512/labels\n",
            "3513\n",
            "img/23415.png\n",
            "\n",
            "0: 448x640 2 persons, 1 cup, 1 potted plant, 1 laptop, 20.4ms\n",
            "Speed: 3.2ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3513\u001b[0m\n",
            "1 label saved to runs/detect/train3513/labels\n",
            "3514\n",
            "img/46753.png\n",
            "\n",
            "0: 640x640 12 persons, 19.6ms\n",
            "Speed: 5.3ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3514\u001b[0m\n",
            "1 label saved to runs/detect/train3514/labels\n",
            "3515\n",
            "img/38029.png\n",
            "\n",
            "0: 448x640 1 car, 1 truck, 19.9ms\n",
            "Speed: 3.2ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3515\u001b[0m\n",
            "1 label saved to runs/detect/train3515/labels\n",
            "3516\n",
            "img/12956.png\n",
            "\n",
            "0: 416x640 5 persons, 20.1ms\n",
            "Speed: 3.5ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3516\u001b[0m\n",
            "1 label saved to runs/detect/train3516/labels\n",
            "3517\n",
            "img/78923.png\n",
            "\n",
            "0: 416x640 1 person, 1 dog, 18.8ms\n",
            "Speed: 2.8ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3517\u001b[0m\n",
            "1 label saved to runs/detect/train3517/labels\n",
            "3518\n",
            "img/96524.png\n",
            "\n",
            "0: 448x640 2 persons, 2 cell phones, 29.1ms\n",
            "Speed: 3.0ms preprocess, 29.1ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3518\u001b[0m\n",
            "1 label saved to runs/detect/train3518/labels\n",
            "3519\n",
            "img/47591.png\n",
            "\n",
            "0: 448x640 1 person, 1 dining table, 20.6ms\n",
            "Speed: 3.7ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3519\u001b[0m\n",
            "1 label saved to runs/detect/train3519/labels\n",
            "3520\n",
            "img/45023.png\n",
            "\n",
            "0: 448x640 1 person, 22.2ms\n",
            "Speed: 2.9ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3520\u001b[0m\n",
            "1 label saved to runs/detect/train3520/labels\n",
            "3521\n",
            "img/62851.png\n",
            "\n",
            "0: 640x512 1 person, 21.2ms\n",
            "Speed: 3.8ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3521\u001b[0m\n",
            "1 label saved to runs/detect/train3521/labels\n",
            "3522\n",
            "img/43217.png\n",
            "\n",
            "0: 640x448 1 person, 1 tie, 21.2ms\n",
            "Speed: 2.7ms preprocess, 21.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3522\u001b[0m\n",
            "1 label saved to runs/detect/train3522/labels\n",
            "3523\n",
            "img/69035.png\n",
            "\n",
            "0: 448x640 9 persons, 1 cake, 26.2ms\n",
            "Speed: 2.8ms preprocess, 26.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3523\u001b[0m\n",
            "1 label saved to runs/detect/train3523/labels\n",
            "3524\n",
            "img/18432.png\n",
            "\n",
            "0: 416x640 (no detections), 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3524\u001b[0m\n",
            "0 label saved to runs/detect/train3524/labels\n",
            "3525\n",
            "img/21348.png\n",
            "\n",
            "0: 448x640 2 persons, 1 car, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3525\u001b[0m\n",
            "1 label saved to runs/detect/train3525/labels\n",
            "3526\n",
            "img/04791.png\n",
            "\n",
            "0: 640x512 8 persons, 1 tie, 18.9ms\n",
            "Speed: 2.9ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3526\u001b[0m\n",
            "1 label saved to runs/detect/train3526/labels\n",
            "3527\n",
            "img/67980.png\n",
            "\n",
            "0: 640x576 3 persons, 1 tie, 1 cell phone, 18.8ms\n",
            "Speed: 3.8ms preprocess, 18.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3527\u001b[0m\n",
            "1 label saved to runs/detect/train3527/labels\n",
            "3528\n",
            "img/71358.png\n",
            "\n",
            "0: 448x640 1 person, 19.6ms\n",
            "Speed: 3.2ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3528\u001b[0m\n",
            "1 label saved to runs/detect/train3528/labels\n",
            "3529\n",
            "img/14650.png\n",
            "\n",
            "0: 448x640 4 persons, 19.4ms\n",
            "Speed: 2.9ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3529\u001b[0m\n",
            "1 label saved to runs/detect/train3529/labels\n",
            "3530\n",
            "img/34207.png\n",
            "\n",
            "0: 640x448 1 person, 1 tie, 17.7ms\n",
            "Speed: 3.1ms preprocess, 17.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3530\u001b[0m\n",
            "1 label saved to runs/detect/train3530/labels\n",
            "3531\n",
            "img/76142.png\n",
            "\n",
            "0: 640x448 1 person, 19.6ms\n",
            "Speed: 3.0ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3531\u001b[0m\n",
            "1 label saved to runs/detect/train3531/labels\n",
            "3532\n",
            "img/15936.png\n",
            "\n",
            "0: 640x448 1 person, 18.0ms\n",
            "Speed: 3.9ms preprocess, 18.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3532\u001b[0m\n",
            "1 label saved to runs/detect/train3532/labels\n",
            "3533\n",
            "img/97403.png\n",
            "\n",
            "0: 640x480 2 persons, 19.8ms\n",
            "Speed: 2.7ms preprocess, 19.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3533\u001b[0m\n",
            "1 label saved to runs/detect/train3533/labels\n",
            "3534\n",
            "img/58947.png\n",
            "\n",
            "0: 640x480 1 person, 1 book, 18.3ms\n",
            "Speed: 2.6ms preprocess, 18.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3534\u001b[0m\n",
            "1 label saved to runs/detect/train3534/labels\n",
            "3535\n",
            "img/53274.png\n",
            "\n",
            "0: 576x640 1 person, 1 backpack, 20.4ms\n",
            "Speed: 3.9ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3535\u001b[0m\n",
            "1 label saved to runs/detect/train3535/labels\n",
            "3536\n",
            "img/94078.png\n",
            "\n",
            "0: 448x640 3 persons, 2 ties, 1 baseball bat, 18.8ms\n",
            "Speed: 3.0ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3536\u001b[0m\n",
            "1 label saved to runs/detect/train3536/labels\n",
            "3537\n",
            "img/18507.png\n",
            "\n",
            "0: 640x384 1 cat, 20.9ms\n",
            "Speed: 3.1ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3537\u001b[0m\n",
            "1 label saved to runs/detect/train3537/labels\n",
            "3538\n",
            "img/50492.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 22.0ms\n",
            "Speed: 3.4ms preprocess, 22.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3538\u001b[0m\n",
            "1 label saved to runs/detect/train3538/labels\n",
            "3539\n",
            "img/52706.png\n",
            "\n",
            "0: 640x576 5 persons, 21.1ms\n",
            "Speed: 3.6ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3539\u001b[0m\n",
            "1 label saved to runs/detect/train3539/labels\n",
            "3540\n",
            "img/10759.png\n",
            "\n",
            "0: 448x640 1 person, 24.9ms\n",
            "Speed: 3.6ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3540\u001b[0m\n",
            "1 label saved to runs/detect/train3540/labels\n",
            "3541\n",
            "img/70429.png\n",
            "\n",
            "0: 448x640 1 person, 19.1ms\n",
            "Speed: 2.9ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3541\u001b[0m\n",
            "1 label saved to runs/detect/train3541/labels\n",
            "3542\n",
            "img/38956.png\n",
            "\n",
            "0: 448x640 1 person, 1 banana, 18.6ms\n",
            "Speed: 3.3ms preprocess, 18.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3542\u001b[0m\n",
            "1 label saved to runs/detect/train3542/labels\n",
            "3543\n",
            "img/64029.png\n",
            "\n",
            "0: 448x640 1 dog, 27.8ms\n",
            "Speed: 3.1ms preprocess, 27.8ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3543\u001b[0m\n",
            "1 label saved to runs/detect/train3543/labels\n",
            "3544\n",
            "img/01256.png\n",
            "\n",
            "0: 640x640 1 person, 1 dog, 22.2ms\n",
            "Speed: 3.4ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3544\u001b[0m\n",
            "1 label saved to runs/detect/train3544/labels\n",
            "3545\n",
            "img/56907.png\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 1 handbag, 1 cup, 1 potted plant, 29.3ms\n",
            "Speed: 3.3ms preprocess, 29.3ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3545\u001b[0m\n",
            "1 label saved to runs/detect/train3545/labels\n",
            "3546\n",
            "img/47530.png\n",
            "\n",
            "0: 448x640 6 persons, 1 tie, 25.0ms\n",
            "Speed: 2.9ms preprocess, 25.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3546\u001b[0m\n",
            "1 label saved to runs/detect/train3546/labels\n",
            "3547\n",
            "img/54018.png\n",
            "\n",
            "0: 640x448 1 person, 1 chair, 1 couch, 34.9ms\n",
            "Speed: 3.2ms preprocess, 34.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3547\u001b[0m\n",
            "1 label saved to runs/detect/train3547/labels\n",
            "3548\n",
            "img/49671.png\n",
            "\n",
            "0: 448x640 2 persons, 30.4ms\n",
            "Speed: 3.3ms preprocess, 30.4ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3548\u001b[0m\n",
            "1 label saved to runs/detect/train3548/labels\n",
            "3549\n",
            "img/89465.png\n",
            "\n",
            "0: 448x640 1 person, 25.3ms\n",
            "Speed: 3.1ms preprocess, 25.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3549\u001b[0m\n",
            "1 label saved to runs/detect/train3549/labels\n",
            "3550\n",
            "img/30465.png\n",
            "\n",
            "0: 448x640 1 dog, 1 teddy bear, 25.9ms\n",
            "Speed: 3.9ms preprocess, 25.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3550\u001b[0m\n",
            "1 label saved to runs/detect/train3550/labels\n",
            "3551\n",
            "img/21598.png\n",
            "\n",
            "0: 320x640 17 persons, 2 bicycles, 33.5ms\n",
            "Speed: 2.4ms preprocess, 33.5ms inference, 4.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3551\u001b[0m\n",
            "1 label saved to runs/detect/train3551/labels\n",
            "3552\n",
            "img/13548.png\n",
            "\n",
            "0: 448x640 1 person, 24.5ms\n",
            "Speed: 3.5ms preprocess, 24.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3552\u001b[0m\n",
            "1 label saved to runs/detect/train3552/labels\n",
            "3553\n",
            "img/30682.png\n",
            "\n",
            "0: 448x640 1 person, 30.1ms\n",
            "Speed: 2.9ms preprocess, 30.1ms inference, 3.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3553\u001b[0m\n",
            "1 label saved to runs/detect/train3553/labels\n",
            "3554\n",
            "img/37642.png\n",
            "\n",
            "0: 448x640 6 persons, 22.6ms\n",
            "Speed: 3.1ms preprocess, 22.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3554\u001b[0m\n",
            "1 label saved to runs/detect/train3554/labels\n",
            "3555\n",
            "img/70169.png\n",
            "\n",
            "0: 640x448 2 persons, 26.3ms\n",
            "Speed: 2.6ms preprocess, 26.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3555\u001b[0m\n",
            "1 label saved to runs/detect/train3555/labels\n",
            "3556\n",
            "img/61825.png\n",
            "\n",
            "0: 448x640 1 dog, 1 baseball bat, 2 books, 30.0ms\n",
            "Speed: 6.5ms preprocess, 30.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3556\u001b[0m\n",
            "1 label saved to runs/detect/train3556/labels\n",
            "3557\n",
            "img/16720.png\n",
            "\n",
            "0: 448x640 1 person, 27.5ms\n",
            "Speed: 3.0ms preprocess, 27.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3557\u001b[0m\n",
            "1 label saved to runs/detect/train3557/labels\n",
            "3558\n",
            "img/87619.png\n",
            "\n",
            "0: 640x448 1 person, 1 handbag, 1 baseball glove, 26.1ms\n",
            "Speed: 3.2ms preprocess, 26.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3558\u001b[0m\n",
            "1 label saved to runs/detect/train3558/labels\n",
            "3559\n",
            "img/35781.png\n",
            "\n",
            "0: 448x640 2 persons, 2 chairs, 1 couch, 23.8ms\n",
            "Speed: 2.7ms preprocess, 23.8ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3559\u001b[0m\n",
            "1 label saved to runs/detect/train3559/labels\n",
            "3560\n",
            "img/25438.png\n",
            "\n",
            "0: 448x640 1 person, 1 car, 1 truck, 22.9ms\n",
            "Speed: 3.0ms preprocess, 22.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3560\u001b[0m\n",
            "1 label saved to runs/detect/train3560/labels\n",
            "3561\n",
            "img/35674.png\n",
            "\n",
            "0: 640x640 2 persons, 1 bed, 23.2ms\n",
            "Speed: 4.3ms preprocess, 23.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3561\u001b[0m\n",
            "1 label saved to runs/detect/train3561/labels\n",
            "3562\n",
            "img/14260.png\n",
            "\n",
            "0: 640x640 2 cats, 24.9ms\n",
            "Speed: 3.6ms preprocess, 24.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3562\u001b[0m\n",
            "1 label saved to runs/detect/train3562/labels\n",
            "3563\n",
            "img/54016.png\n",
            "\n",
            "0: 640x448 1 person, 38.1ms\n",
            "Speed: 3.2ms preprocess, 38.1ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3563\u001b[0m\n",
            "1 label saved to runs/detect/train3563/labels\n",
            "3564\n",
            "img/86597.png\n",
            "\n",
            "0: 448x640 6 sheeps, 4 cows, 25.1ms\n",
            "Speed: 3.0ms preprocess, 25.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3564\u001b[0m\n",
            "1 label saved to runs/detect/train3564/labels\n",
            "3565\n",
            "img/84709.png\n",
            "\n",
            "0: 640x384 2 persons, 1 bed, 34.7ms\n",
            "Speed: 2.3ms preprocess, 34.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3565\u001b[0m\n",
            "1 label saved to runs/detect/train3565/labels\n",
            "3566\n",
            "img/14296.png\n",
            "\n",
            "0: 448x640 1 person, 28.6ms\n",
            "Speed: 4.8ms preprocess, 28.6ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3566\u001b[0m\n",
            "1 label saved to runs/detect/train3566/labels\n",
            "3567\n",
            "img/01487.png\n",
            "\n",
            "0: 640x448 2 persons, 33.3ms\n",
            "Speed: 2.5ms preprocess, 33.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3567\u001b[0m\n",
            "1 label saved to runs/detect/train3567/labels\n",
            "3568\n",
            "img/92407.png\n",
            "\n",
            "0: 448x640 1 person, 1 frisbee, 27.6ms\n",
            "Speed: 3.8ms preprocess, 27.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3568\u001b[0m\n",
            "1 label saved to runs/detect/train3568/labels\n",
            "3569\n",
            "img/58490.png\n",
            "\n",
            "0: 448x640 1 cat, 25.5ms\n",
            "Speed: 3.0ms preprocess, 25.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3569\u001b[0m\n",
            "1 label saved to runs/detect/train3569/labels\n",
            "3570\n",
            "img/49075.png\n",
            "\n",
            "0: 448x640 3 umbrellas, 1 kite, 23.5ms\n",
            "Speed: 3.0ms preprocess, 23.5ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3570\u001b[0m\n",
            "1 label saved to runs/detect/train3570/labels\n",
            "3571\n",
            "img/07389.png\n",
            "\n",
            "0: 640x640 2 dogs, 24.3ms\n",
            "Speed: 4.3ms preprocess, 24.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3571\u001b[0m\n",
            "1 label saved to runs/detect/train3571/labels\n",
            "3572\n",
            "img/20718.png\n",
            "\n",
            "0: 640x576 2 persons, 2 dogs, 1 cell phone, 29.7ms\n",
            "Speed: 3.6ms preprocess, 29.7ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3572\u001b[0m\n",
            "1 label saved to runs/detect/train3572/labels\n",
            "3573\n",
            "img/75839.png\n",
            "\n",
            "0: 448x640 2 persons, 35.7ms\n",
            "Speed: 2.8ms preprocess, 35.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3573\u001b[0m\n",
            "1 label saved to runs/detect/train3573/labels\n",
            "3574\n",
            "img/03718.png\n",
            "\n",
            "0: 640x480 1 person, 4 sheeps, 31.0ms\n",
            "Speed: 4.8ms preprocess, 31.0ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3574\u001b[0m\n",
            "1 label saved to runs/detect/train3574/labels\n",
            "3575\n",
            "img/36571.png\n",
            "\n",
            "0: 416x640 14 persons, 2 handbags, 38.0ms\n",
            "Speed: 2.7ms preprocess, 38.0ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3575\u001b[0m\n",
            "1 label saved to runs/detect/train3575/labels\n",
            "3576\n",
            "img/15420.png\n",
            "\n",
            "0: 640x480 6 persons, 1 wine glass, 42.5ms\n",
            "Speed: 3.5ms preprocess, 42.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3576\u001b[0m\n",
            "1 label saved to runs/detect/train3576/labels\n",
            "3577\n",
            "img/28546.png\n",
            "\n",
            "0: 448x640 1 person, 1 banana, 20.9ms\n",
            "Speed: 3.0ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3577\u001b[0m\n",
            "1 label saved to runs/detect/train3577/labels\n",
            "3578\n",
            "img/25197.png\n",
            "\n",
            "0: 448x640 11 persons, 18.5ms\n",
            "Speed: 2.8ms preprocess, 18.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3578\u001b[0m\n",
            "1 label saved to runs/detect/train3578/labels\n",
            "3579\n",
            "img/06482.png\n",
            "\n",
            "0: 416x640 1 person, 1 couch, 20.4ms\n",
            "Speed: 3.6ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3579\u001b[0m\n",
            "1 label saved to runs/detect/train3579/labels\n",
            "3580\n",
            "img/82413.png\n",
            "\n",
            "0: 640x544 1 person, 20.2ms\n",
            "Speed: 3.5ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3580\u001b[0m\n",
            "1 label saved to runs/detect/train3580/labels\n",
            "3581\n",
            "img/62854.png\n",
            "\n",
            "0: 448x640 1 person, 23.2ms\n",
            "Speed: 4.0ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3581\u001b[0m\n",
            "1 label saved to runs/detect/train3581/labels\n",
            "3582\n",
            "img/79352.png\n",
            "\n",
            "0: 640x448 1 person, 20.0ms\n",
            "Speed: 2.6ms preprocess, 20.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3582\u001b[0m\n",
            "1 label saved to runs/detect/train3582/labels\n",
            "3583\n",
            "img/05847.png\n",
            "\n",
            "0: 512x640 1 person, 20.8ms\n",
            "Speed: 4.3ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3583\u001b[0m\n",
            "1 label saved to runs/detect/train3583/labels\n",
            "3584\n",
            "img/03257.png\n",
            "\n",
            "0: 640x448 1 person, 22.5ms\n",
            "Speed: 3.9ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3584\u001b[0m\n",
            "1 label saved to runs/detect/train3584/labels\n",
            "3585\n",
            "img/34751.png\n",
            "\n",
            "0: 448x640 1 person, 1 scissors, 1 toothbrush, 27.0ms\n",
            "Speed: 2.0ms preprocess, 27.0ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3585\u001b[0m\n",
            "1 label saved to runs/detect/train3585/labels\n",
            "3586\n",
            "img/96307.png\n",
            "\n",
            "0: 448x640 1 person, 17.7ms\n",
            "Speed: 3.5ms preprocess, 17.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3586\u001b[0m\n",
            "1 label saved to runs/detect/train3586/labels\n",
            "3587\n",
            "img/52108.png\n",
            "\n",
            "0: 640x448 1 person, 20.3ms\n",
            "Speed: 2.5ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3587\u001b[0m\n",
            "1 label saved to runs/detect/train3587/labels\n",
            "3588\n",
            "img/57608.png\n",
            "\n",
            "0: 640x640 1 cat, 19.4ms\n",
            "Speed: 4.1ms preprocess, 19.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3588\u001b[0m\n",
            "1 label saved to runs/detect/train3588/labels\n",
            "3589\n",
            "img/74953.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 20.5ms\n",
            "Speed: 2.8ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3589\u001b[0m\n",
            "1 label saved to runs/detect/train3589/labels\n",
            "3590\n",
            "img/78315.png\n",
            "\n",
            "0: 512x640 1 person, 18.7ms\n",
            "Speed: 4.6ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3590\u001b[0m\n",
            "1 label saved to runs/detect/train3590/labels\n",
            "3591\n",
            "img/31085.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bed, 19.2ms\n",
            "Speed: 3.0ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3591\u001b[0m\n",
            "1 label saved to runs/detect/train3591/labels\n",
            "3592\n",
            "img/97258.png\n",
            "\n",
            "0: 448x640 1 person, 18.3ms\n",
            "Speed: 4.1ms preprocess, 18.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3592\u001b[0m\n",
            "1 label saved to runs/detect/train3592/labels\n",
            "3593\n",
            "img/59321.png\n",
            "\n",
            "0: 640x448 1 person, 19.2ms\n",
            "Speed: 3.7ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3593\u001b[0m\n",
            "1 label saved to runs/detect/train3593/labels\n",
            "3594\n",
            "img/12063.png\n",
            "\n",
            "0: 448x640 1 dog, 2 cows, 22.0ms\n",
            "Speed: 2.6ms preprocess, 22.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3594\u001b[0m\n",
            "1 label saved to runs/detect/train3594/labels\n",
            "3595\n",
            "img/59248.png\n",
            "\n",
            "0: 640x128 5 persons, 20.9ms\n",
            "Speed: 1.3ms preprocess, 20.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 128)\n",
            "Results saved to \u001b[1mruns/detect/train3595\u001b[0m\n",
            "1 label saved to runs/detect/train3595/labels\n",
            "3596\n",
            "img/84592.png\n",
            "\n",
            "0: 480x640 1 dog, 23.1ms\n",
            "Speed: 3.1ms preprocess, 23.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3596\u001b[0m\n",
            "1 label saved to runs/detect/train3596/labels\n",
            "3597\n",
            "img/61834.png\n",
            "\n",
            "0: 640x384 2 persons, 1 tie, 23.7ms\n",
            "Speed: 2.1ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3597\u001b[0m\n",
            "1 label saved to runs/detect/train3597/labels\n",
            "3598\n",
            "img/30456.png\n",
            "\n",
            "0: 640x448 1 person, 19.8ms\n",
            "Speed: 3.1ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3598\u001b[0m\n",
            "1 label saved to runs/detect/train3598/labels\n",
            "3599\n",
            "img/89507.png\n",
            "\n",
            "0: 448x640 1 person, 19.5ms\n",
            "Speed: 2.9ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3599\u001b[0m\n",
            "1 label saved to runs/detect/train3599/labels\n",
            "3600\n",
            "img/02674.png\n",
            "\n",
            "0: 640x352 1 person, 1 car, 1 chair, 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3600\u001b[0m\n",
            "1 label saved to runs/detect/train3600/labels\n",
            "3601\n",
            "img/36179.png\n",
            "\n",
            "0: 640x544 1 person, 20.6ms\n",
            "Speed: 3.7ms preprocess, 20.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3601\u001b[0m\n",
            "1 label saved to runs/detect/train3601/labels\n",
            "3602\n",
            "img/14072.png\n",
            "\n",
            "0: 640x480 2 persons, 19.9ms\n",
            "Speed: 3.1ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3602\u001b[0m\n",
            "1 label saved to runs/detect/train3602/labels\n",
            "3603\n",
            "img/43920.png\n",
            "\n",
            "0: 448x640 4 persons, 20.1ms\n",
            "Speed: 3.0ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3603\u001b[0m\n",
            "1 label saved to runs/detect/train3603/labels\n",
            "3604\n",
            "img/27913.png\n",
            "\n",
            "0: 448x640 1 person, 19.7ms\n",
            "Speed: 3.7ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3604\u001b[0m\n",
            "1 label saved to runs/detect/train3604/labels\n",
            "3605\n",
            "img/07268.png\n",
            "\n",
            "0: 640x448 3 persons, 1 car, 22.9ms\n",
            "Speed: 3.2ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3605\u001b[0m\n",
            "1 label saved to runs/detect/train3605/labels\n",
            "3606\n",
            "img/18790.png\n",
            "\n",
            "0: 448x640 2 persons, 2 chairs, 30.1ms\n",
            "Speed: 2.8ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3606\u001b[0m\n",
            "1 label saved to runs/detect/train3606/labels\n",
            "3607\n",
            "img/98412.png\n",
            "\n",
            "0: 640x224 3 persons, 19.9ms\n",
            "Speed: 1.6ms preprocess, 19.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 224)\n",
            "Results saved to \u001b[1mruns/detect/train3607\u001b[0m\n",
            "1 label saved to runs/detect/train3607/labels\n",
            "3608\n",
            "img/72145.png\n",
            "\n",
            "0: 448x640 9 sheeps, 5 cows, 19.7ms\n",
            "Speed: 5.8ms preprocess, 19.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3608\u001b[0m\n",
            "1 label saved to runs/detect/train3608/labels\n",
            "3609\n",
            "img/86947.png\n",
            "\n",
            "0: 640x640 3 persons, 2 surfboards, 18.4ms\n",
            "Speed: 4.0ms preprocess, 18.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3609\u001b[0m\n",
            "1 label saved to runs/detect/train3609/labels\n",
            "3610\n",
            "img/08372.png\n",
            "\n",
            "0: 640x544 1 person, 21.1ms\n",
            "Speed: 3.0ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3610\u001b[0m\n",
            "1 label saved to runs/detect/train3610/labels\n",
            "3611\n",
            "img/79850.png\n",
            "\n",
            "0: 640x416 1 person, 19.1ms\n",
            "Speed: 3.2ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3611\u001b[0m\n",
            "1 label saved to runs/detect/train3611/labels\n",
            "3612\n",
            "img/16389.png\n",
            "\n",
            "0: 640x448 1 person, 1 cup, 1 teddy bear, 21.8ms\n",
            "Speed: 2.7ms preprocess, 21.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3612\u001b[0m\n",
            "1 label saved to runs/detect/train3612/labels\n",
            "3613\n",
            "img/75941.png\n",
            "\n",
            "0: 640x448 1 person, 28.4ms\n",
            "Speed: 2.8ms preprocess, 28.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3613\u001b[0m\n",
            "1 label saved to runs/detect/train3613/labels\n",
            "3614\n",
            "img/26543.png\n",
            "\n",
            "0: 640x160 4 persons, 3 ties, 21.6ms\n",
            "Speed: 1.3ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 160)\n",
            "Results saved to \u001b[1mruns/detect/train3614\u001b[0m\n",
            "1 label saved to runs/detect/train3614/labels\n",
            "3615\n",
            "img/28391.png\n",
            "\n",
            "0: 640x448 1 person, 19.7ms\n",
            "Speed: 6.8ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3615\u001b[0m\n",
            "1 label saved to runs/detect/train3615/labels\n",
            "3616\n",
            "img/17459.png\n",
            "\n",
            "0: 448x640 1 person, 19.6ms\n",
            "Speed: 3.4ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3616\u001b[0m\n",
            "1 label saved to runs/detect/train3616/labels\n",
            "3617\n",
            "img/05749.png\n",
            "\n",
            "0: 448x640 2 persons, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3617\u001b[0m\n",
            "1 label saved to runs/detect/train3617/labels\n",
            "3618\n",
            "img/74956.png\n",
            "\n",
            "0: 480x640 (no detections), 19.2ms\n",
            "Speed: 3.4ms preprocess, 19.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3618\u001b[0m\n",
            "0 label saved to runs/detect/train3618/labels\n",
            "3619\n",
            "img/81457.png\n",
            "\n",
            "0: 448x640 12 persons, 22.4ms\n",
            "Speed: 3.1ms preprocess, 22.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3619\u001b[0m\n",
            "1 label saved to runs/detect/train3619/labels\n",
            "3620\n",
            "img/39408.png\n",
            "\n",
            "0: 448x640 1 person, 21.5ms\n",
            "Speed: 2.8ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3620\u001b[0m\n",
            "1 label saved to runs/detect/train3620/labels\n",
            "3621\n",
            "img/04923.png\n",
            "\n",
            "0: 640x480 1 person, 1 cell phone, 19.7ms\n",
            "Speed: 3.4ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3621\u001b[0m\n",
            "1 label saved to runs/detect/train3621/labels\n",
            "3622\n",
            "img/28657.png\n",
            "\n",
            "0: 640x480 1 dog, 27.7ms\n",
            "Speed: 3.4ms preprocess, 27.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3622\u001b[0m\n",
            "1 label saved to runs/detect/train3622/labels\n",
            "3623\n",
            "img/35814.png\n",
            "\n",
            "0: 448x640 3 persons, 19.6ms\n",
            "Speed: 3.7ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3623\u001b[0m\n",
            "1 label saved to runs/detect/train3623/labels\n",
            "3624\n",
            "img/26087.png\n",
            "\n",
            "0: 448x640 (no detections), 23.5ms\n",
            "Speed: 3.1ms preprocess, 23.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3624\u001b[0m\n",
            "0 label saved to runs/detect/train3624/labels\n",
            "3625\n",
            "img/28756.png\n",
            "\n",
            "0: 448x640 1 person, 1 sports ball, 18.0ms\n",
            "Speed: 3.4ms preprocess, 18.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3625\u001b[0m\n",
            "1 label saved to runs/detect/train3625/labels\n",
            "3626\n",
            "img/59832.png\n",
            "\n",
            "0: 640x480 3 persons, 1 cow, 20.0ms\n",
            "Speed: 3.0ms preprocess, 20.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3626\u001b[0m\n",
            "1 label saved to runs/detect/train3626/labels\n",
            "3627\n",
            "img/07912.png\n",
            "\n",
            "0: 384x640 (no detections), 22.5ms\n",
            "Speed: 3.1ms preprocess, 22.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3627\u001b[0m\n",
            "0 label saved to runs/detect/train3627/labels\n",
            "3628\n",
            "img/17905.png\n",
            "\n",
            "0: 448x640 1 dog, 20.8ms\n",
            "Speed: 4.0ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3628\u001b[0m\n",
            "1 label saved to runs/detect/train3628/labels\n",
            "3629\n",
            "img/96857.png\n",
            "\n",
            "0: 640x480 1 person, 20.4ms\n",
            "Speed: 2.8ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3629\u001b[0m\n",
            "1 label saved to runs/detect/train3629/labels\n",
            "3630\n",
            "img/04768.png\n",
            "\n",
            "0: 640x448 1 person, 21.4ms\n",
            "Speed: 2.6ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3630\u001b[0m\n",
            "1 label saved to runs/detect/train3630/labels\n",
            "3631\n",
            "img/45687.png\n",
            "\n",
            "0: 640x320 14 persons, 2 birds, 2 backpacks, 21.1ms\n",
            "Speed: 2.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3631\u001b[0m\n",
            "1 label saved to runs/detect/train3631/labels\n",
            "3632\n",
            "img/86730.png\n",
            "\n",
            "0: 640x576 (no detections), 19.6ms\n",
            "Speed: 4.7ms preprocess, 19.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3632\u001b[0m\n",
            "0 label saved to runs/detect/train3632/labels\n",
            "3633\n",
            "img/83602.png\n",
            "\n",
            "0: 448x640 1 person, 19.0ms\n",
            "Speed: 2.9ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3633\u001b[0m\n",
            "1 label saved to runs/detect/train3633/labels\n",
            "3634\n",
            "img/30692.png\n",
            "\n",
            "0: 448x640 (no detections), 18.2ms\n",
            "Speed: 3.8ms preprocess, 18.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3634\u001b[0m\n",
            "0 label saved to runs/detect/train3634/labels\n",
            "3635\n",
            "img/83271.png\n",
            "\n",
            "0: 448x640 2 persons, 24.5ms\n",
            "Speed: 2.4ms preprocess, 24.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3635\u001b[0m\n",
            "1 label saved to runs/detect/train3635/labels\n",
            "3636\n",
            "img/49802.png\n",
            "\n",
            "0: 640x448 2 persons, 1 toilet, 19.2ms\n",
            "Speed: 3.6ms preprocess, 19.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3636\u001b[0m\n",
            "1 label saved to runs/detect/train3636/labels\n",
            "3637\n",
            "img/10478.png\n",
            "\n",
            "0: 352x640 (no detections), 21.5ms\n",
            "Speed: 2.6ms preprocess, 21.5ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3637\u001b[0m\n",
            "0 label saved to runs/detect/train3637/labels\n",
            "3638\n",
            "img/39056.png\n",
            "\n",
            "0: 448x640 7 persons, 1 backpack, 7 chairs, 2 dining tables, 20.2ms\n",
            "Speed: 3.4ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3638\u001b[0m\n",
            "1 label saved to runs/detect/train3638/labels\n",
            "3639\n",
            "img/51462.png\n",
            "\n",
            "0: 448x640 1 person, 18.6ms\n",
            "Speed: 3.0ms preprocess, 18.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3639\u001b[0m\n",
            "1 label saved to runs/detect/train3639/labels\n",
            "3640\n",
            "img/60398.png\n",
            "\n",
            "0: 448x640 2 persons, 19.3ms\n",
            "Speed: 3.8ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3640\u001b[0m\n",
            "1 label saved to runs/detect/train3640/labels\n",
            "3641\n",
            "img/86391.png\n",
            "\n",
            "0: 640x480 (no detections), 21.9ms\n",
            "Speed: 2.9ms preprocess, 21.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3641\u001b[0m\n",
            "0 label saved to runs/detect/train3641/labels\n",
            "3642\n",
            "img/40239.png\n",
            "\n",
            "0: 640x448 4 persons, 2 ties, 19.4ms\n",
            "Speed: 3.5ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3642\u001b[0m\n",
            "1 label saved to runs/detect/train3642/labels\n",
            "3643\n",
            "img/57198.png\n",
            "\n",
            "0: 448x640 3 persons, 26.1ms\n",
            "Speed: 3.0ms preprocess, 26.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3643\u001b[0m\n",
            "1 label saved to runs/detect/train3643/labels\n",
            "3644\n",
            "img/41908.png\n",
            "\n",
            "0: 448x640 10 persons, 1 handbag, 19.4ms\n",
            "Speed: 2.9ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3644\u001b[0m\n",
            "1 label saved to runs/detect/train3644/labels\n",
            "3645\n",
            "img/90534.png\n",
            "\n",
            "0: 448x640 1 person, 20.9ms\n",
            "Speed: 3.8ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3645\u001b[0m\n",
            "1 label saved to runs/detect/train3645/labels\n",
            "3646\n",
            "img/35764.png\n",
            "\n",
            "0: 448x640 2 persons, 1 tie, 17.8ms\n",
            "Speed: 2.7ms preprocess, 17.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3646\u001b[0m\n",
            "1 label saved to runs/detect/train3646/labels\n",
            "3647\n",
            "img/79125.png\n",
            "\n",
            "0: 448x640 1 person, 1 knife, 23.8ms\n",
            "Speed: 2.8ms preprocess, 23.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3647\u001b[0m\n",
            "1 label saved to runs/detect/train3647/labels\n",
            "3648\n",
            "img/38641.png\n",
            "\n",
            "0: 640x512 (no detections), 20.1ms\n",
            "Speed: 4.0ms preprocess, 20.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3648\u001b[0m\n",
            "0 label saved to runs/detect/train3648/labels\n",
            "3649\n",
            "img/24069.png\n",
            "\n",
            "0: 480x640 4 persons, 19.9ms\n",
            "Speed: 3.0ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3649\u001b[0m\n",
            "1 label saved to runs/detect/train3649/labels\n",
            "3650\n",
            "img/27591.png\n",
            "\n",
            "0: 640x480 1 person, 2 ties, 22.6ms\n",
            "Speed: 2.9ms preprocess, 22.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3650\u001b[0m\n",
            "1 label saved to runs/detect/train3650/labels\n",
            "3651\n",
            "img/17425.png\n",
            "\n",
            "0: 448x640 1 person, 20.6ms\n",
            "Speed: 3.4ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3651\u001b[0m\n",
            "1 label saved to runs/detect/train3651/labels\n",
            "3652\n",
            "img/38046.png\n",
            "\n",
            "0: 448x640 3 persons, 1 couch, 21.0ms\n",
            "Speed: 3.3ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3652\u001b[0m\n",
            "1 label saved to runs/detect/train3652/labels\n",
            "3653\n",
            "img/49126.png\n",
            "\n",
            "0: 448x640 4 persons, 17.9ms\n",
            "Speed: 4.0ms preprocess, 17.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3653\u001b[0m\n",
            "1 label saved to runs/detect/train3653/labels\n",
            "3654\n",
            "img/04217.png\n",
            "\n",
            "0: 448x640 1 person, 1 cell phone, 21.4ms\n",
            "Speed: 3.8ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3654\u001b[0m\n",
            "1 label saved to runs/detect/train3654/labels\n",
            "3655\n",
            "img/57249.png\n",
            "\n",
            "0: 640x416 1 person, 23.4ms\n",
            "Speed: 3.0ms preprocess, 23.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3655\u001b[0m\n",
            "1 label saved to runs/detect/train3655/labels\n",
            "3656\n",
            "img/34075.png\n",
            "\n",
            "0: 544x640 1 person, 18.8ms\n",
            "Speed: 3.9ms preprocess, 18.8ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3656\u001b[0m\n",
            "1 label saved to runs/detect/train3656/labels\n",
            "3657\n",
            "img/01694.png\n",
            "\n",
            "0: 640x480 4 persons, 1 horse, 6 cows, 21.1ms\n",
            "Speed: 2.7ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3657\u001b[0m\n",
            "1 label saved to runs/detect/train3657/labels\n",
            "3658\n",
            "img/67180.png\n",
            "\n",
            "0: 512x640 1 person, 19.2ms\n",
            "Speed: 3.2ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3658\u001b[0m\n",
            "1 label saved to runs/detect/train3658/labels\n",
            "3659\n",
            "img/73159.png\n",
            "\n",
            "0: 448x640 10 persons, 2 chairs, 2 tvs, 19.5ms\n",
            "Speed: 3.6ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3659\u001b[0m\n",
            "1 label saved to runs/detect/train3659/labels\n",
            "3660\n",
            "img/82179.png\n",
            "\n",
            "0: 576x640 1 cat, 1 suitcase, 17.6ms\n",
            "Speed: 3.8ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3660\u001b[0m\n",
            "1 label saved to runs/detect/train3660/labels\n",
            "3661\n",
            "img/81079.png\n",
            "\n",
            "0: 448x640 1 person, 19.4ms\n",
            "Speed: 3.7ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3661\u001b[0m\n",
            "1 label saved to runs/detect/train3661/labels\n",
            "3662\n",
            "img/12953.png\n",
            "\n",
            "0: 448x640 16 persons, 1 handbag, 1 chair, 19.2ms\n",
            "Speed: 2.8ms preprocess, 19.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3662\u001b[0m\n",
            "1 label saved to runs/detect/train3662/labels\n",
            "3663\n",
            "img/70529.png\n",
            "\n",
            "0: 448x640 1 person, 1 bed, 19.0ms\n",
            "Speed: 3.4ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3663\u001b[0m\n",
            "1 label saved to runs/detect/train3663/labels\n",
            "3664\n",
            "img/87621.png\n",
            "\n",
            "0: 640x640 1 person, 1 tie, 1 cup, 2 clocks, 19.1ms\n",
            "Speed: 3.6ms preprocess, 19.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3664\u001b[0m\n",
            "1 label saved to runs/detect/train3664/labels\n",
            "3665\n",
            "img/30872.png\n",
            "\n",
            "0: 640x480 1 person, 2 ties, 18.7ms\n",
            "Speed: 3.3ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3665\u001b[0m\n",
            "1 label saved to runs/detect/train3665/labels\n",
            "3666\n",
            "img/26598.png\n",
            "\n",
            "0: 448x640 1 person, 19.9ms\n",
            "Speed: 3.8ms preprocess, 19.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3666\u001b[0m\n",
            "1 label saved to runs/detect/train3666/labels\n",
            "3667\n",
            "img/36928.png\n",
            "\n",
            "0: 640x480 1 person, 19.2ms\n",
            "Speed: 3.3ms preprocess, 19.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3667\u001b[0m\n",
            "1 label saved to runs/detect/train3667/labels\n",
            "3668\n",
            "img/78921.png\n",
            "\n",
            "0: 480x640 1 bear, 22.6ms\n",
            "Speed: 3.4ms preprocess, 22.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3668\u001b[0m\n",
            "1 label saved to runs/detect/train3668/labels\n",
            "3669\n",
            "img/62510.png\n",
            "\n",
            "0: 448x640 1 person, 19.0ms\n",
            "Speed: 3.0ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3669\u001b[0m\n",
            "1 label saved to runs/detect/train3669/labels\n",
            "3670\n",
            "img/91430.png\n",
            "\n",
            "0: 448x640 4 persons, 2 umbrellas, 2 ties, 1 surfboard, 4 bottles, 1 chair, 19.7ms\n",
            "Speed: 3.0ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3670\u001b[0m\n",
            "1 label saved to runs/detect/train3670/labels\n",
            "3671\n",
            "img/17052.png\n",
            "\n",
            "0: 416x640 2 persons, 28.6ms\n",
            "Speed: 2.6ms preprocess, 28.6ms inference, 2.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3671\u001b[0m\n",
            "1 label saved to runs/detect/train3671/labels\n",
            "3672\n",
            "img/48306.png\n",
            "\n",
            "0: 480x640 1 person, 23.9ms\n",
            "Speed: 3.5ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3672\u001b[0m\n",
            "1 label saved to runs/detect/train3672/labels\n",
            "3673\n",
            "img/29841.png\n",
            "\n",
            "0: 416x640 1 person, 20.0ms\n",
            "Speed: 3.1ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3673\u001b[0m\n",
            "1 label saved to runs/detect/train3673/labels\n",
            "3674\n",
            "img/16824.png\n",
            "\n",
            "0: 640x352 2 persons, 1 tie, 19.8ms\n",
            "Speed: 2.7ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3674\u001b[0m\n",
            "1 label saved to runs/detect/train3674/labels\n",
            "3675\n",
            "img/58642.png\n",
            "\n",
            "0: 448x640 1 person, 19.7ms\n",
            "Speed: 3.7ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3675\u001b[0m\n",
            "1 label saved to runs/detect/train3675/labels\n",
            "3676\n",
            "img/85023.png\n",
            "\n",
            "0: 640x448 1 person, 20.2ms\n",
            "Speed: 2.8ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3676\u001b[0m\n",
            "1 label saved to runs/detect/train3676/labels\n",
            "3677\n",
            "img/89741.png\n",
            "\n",
            "0: 448x640 1 bird, 20.9ms\n",
            "Speed: 4.0ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3677\u001b[0m\n",
            "1 label saved to runs/detect/train3677/labels\n",
            "3678\n",
            "img/31092.png\n",
            "\n",
            "0: 640x352 2 cats, 1 dog, 1 bed, 20.7ms\n",
            "Speed: 3.4ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3678\u001b[0m\n",
            "1 label saved to runs/detect/train3678/labels\n",
            "3679\n",
            "img/26340.png\n",
            "\n",
            "0: 640x448 14 persons, 23.1ms\n",
            "Speed: 3.7ms preprocess, 23.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3679\u001b[0m\n",
            "1 label saved to runs/detect/train3679/labels\n",
            "3680\n",
            "img/07194.png\n",
            "\n",
            "0: 448x640 1 person, 19.5ms\n",
            "Speed: 4.3ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3680\u001b[0m\n",
            "1 label saved to runs/detect/train3680/labels\n",
            "3681\n",
            "img/56123.png\n",
            "\n",
            "0: 448x640 1 person, 18.6ms\n",
            "Speed: 2.9ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3681\u001b[0m\n",
            "1 label saved to runs/detect/train3681/labels\n",
            "3682\n",
            "img/49285.png\n",
            "\n",
            "0: 480x640 1 person, 18.6ms\n",
            "Speed: 2.9ms preprocess, 18.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3682\u001b[0m\n",
            "1 label saved to runs/detect/train3682/labels\n",
            "3683\n",
            "img/63028.png\n",
            "\n",
            "0: 448x640 1 cow, 19.2ms\n",
            "Speed: 3.8ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3683\u001b[0m\n",
            "1 label saved to runs/detect/train3683/labels\n",
            "3684\n",
            "img/98314.png\n",
            "\n",
            "0: 640x416 14 persons, 1 tie, 1 baseball bat, 2 cell phones, 21.8ms\n",
            "Speed: 2.8ms preprocess, 21.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3684\u001b[0m\n",
            "1 label saved to runs/detect/train3684/labels\n",
            "3685\n",
            "img/30185.png\n",
            "\n",
            "0: 640x544 1 person, 21.0ms\n",
            "Speed: 3.8ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3685\u001b[0m\n",
            "1 label saved to runs/detect/train3685/labels\n",
            "3686\n",
            "img/10689.png\n",
            "\n",
            "0: 640x416 3 persons, 22.6ms\n",
            "Speed: 2.8ms preprocess, 22.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3686\u001b[0m\n",
            "1 label saved to runs/detect/train3686/labels\n",
            "3687\n",
            "img/08563.png\n",
            "\n",
            "0: 448x640 9 persons, 1 car, 19.7ms\n",
            "Speed: 3.8ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3687\u001b[0m\n",
            "1 label saved to runs/detect/train3687/labels\n",
            "3688\n",
            "img/36570.png\n",
            "\n",
            "0: 448x640 1 banana, 18.4ms\n",
            "Speed: 2.7ms preprocess, 18.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3688\u001b[0m\n",
            "1 label saved to runs/detect/train3688/labels\n",
            "3689\n",
            "img/71530.png\n",
            "\n",
            "0: 512x640 2 persons, 23.0ms\n",
            "Speed: 3.4ms preprocess, 23.0ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3689\u001b[0m\n",
            "1 label saved to runs/detect/train3689/labels\n",
            "3690\n",
            "img/46925.png\n",
            "\n",
            "0: 544x640 1 person, 21.5ms\n",
            "Speed: 3.6ms preprocess, 21.5ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3690\u001b[0m\n",
            "1 label saved to runs/detect/train3690/labels\n",
            "3691\n",
            "img/58679.png\n",
            "\n",
            "0: 640x544 1 person, 18.3ms\n",
            "Speed: 5.1ms preprocess, 18.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3691\u001b[0m\n",
            "1 label saved to runs/detect/train3691/labels\n",
            "3692\n",
            "img/70123.png\n",
            "\n",
            "0: 640x448 2 persons, 21.4ms\n",
            "Speed: 3.1ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3692\u001b[0m\n",
            "1 label saved to runs/detect/train3692/labels\n",
            "3693\n",
            "img/43520.png\n",
            "\n",
            "0: 448x640 2 persons, 3 cars, 18.8ms\n",
            "Speed: 3.0ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3693\u001b[0m\n",
            "1 label saved to runs/detect/train3693/labels\n",
            "3694\n",
            "img/14657.png\n",
            "\n",
            "0: 640x448 1 person, 19.8ms\n",
            "Speed: 3.5ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3694\u001b[0m\n",
            "1 label saved to runs/detect/train3694/labels\n",
            "3695\n",
            "img/60541.png\n",
            "\n",
            "0: 448x640 1 person, 21.4ms\n",
            "Speed: 2.6ms preprocess, 21.4ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3695\u001b[0m\n",
            "1 label saved to runs/detect/train3695/labels\n",
            "3696\n",
            "img/54690.png\n",
            "\n",
            "0: 480x640 4 persons, 2 ties, 31.6ms\n",
            "Speed: 3.3ms preprocess, 31.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3696\u001b[0m\n",
            "1 label saved to runs/detect/train3696/labels\n",
            "3697\n",
            "img/75241.png\n",
            "\n",
            "0: 448x640 1 person, 1 train, 32.5ms\n",
            "Speed: 2.9ms preprocess, 32.5ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3697\u001b[0m\n",
            "1 label saved to runs/detect/train3697/labels\n",
            "3698\n",
            "img/78623.png\n",
            "\n",
            "0: 448x640 1 car, 1 truck, 20.0ms\n",
            "Speed: 2.8ms preprocess, 20.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3698\u001b[0m\n",
            "1 label saved to runs/detect/train3698/labels\n",
            "3699\n",
            "img/67190.png\n",
            "\n",
            "0: 416x640 (no detections), 25.6ms\n",
            "Speed: 2.6ms preprocess, 25.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3699\u001b[0m\n",
            "0 label saved to runs/detect/train3699/labels\n",
            "3700\n",
            "img/79658.png\n",
            "\n",
            "0: 448x640 1 person, 25.1ms\n",
            "Speed: 2.9ms preprocess, 25.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3700\u001b[0m\n",
            "1 label saved to runs/detect/train3700/labels\n",
            "3701\n",
            "img/68951.png\n",
            "\n",
            "0: 640x480 1 dog, 24.1ms\n",
            "Speed: 3.3ms preprocess, 24.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3701\u001b[0m\n",
            "1 label saved to runs/detect/train3701/labels\n",
            "3702\n",
            "img/72194.png\n",
            "\n",
            "0: 448x640 6 persons, 29.8ms\n",
            "Speed: 6.5ms preprocess, 29.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3702\u001b[0m\n",
            "1 label saved to runs/detect/train3702/labels\n",
            "3703\n",
            "img/64215.png\n",
            "\n",
            "0: 448x640 7 persons, 21.6ms\n",
            "Speed: 2.8ms preprocess, 21.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3703\u001b[0m\n",
            "1 label saved to runs/detect/train3703/labels\n",
            "3704\n",
            "img/19420.png\n",
            "\n",
            "0: 448x640 3 persons, 2 ties, 21.9ms\n",
            "Speed: 2.9ms preprocess, 21.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3704\u001b[0m\n",
            "1 label saved to runs/detect/train3704/labels\n",
            "3705\n",
            "img/04915.png\n",
            "\n",
            "0: 384x640 2 cows, 1 elephant, 27.8ms\n",
            "Speed: 2.8ms preprocess, 27.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3705\u001b[0m\n",
            "1 label saved to runs/detect/train3705/labels\n",
            "3706\n",
            "img/87604.png\n",
            "\n",
            "0: 448x640 1 person, 23.8ms\n",
            "Speed: 2.7ms preprocess, 23.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3706\u001b[0m\n",
            "1 label saved to runs/detect/train3706/labels\n",
            "3707\n",
            "img/71450.png\n",
            "\n",
            "0: 448x640 3 persons, 21.5ms\n",
            "Speed: 3.2ms preprocess, 21.5ms inference, 2.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3707\u001b[0m\n",
            "1 label saved to runs/detect/train3707/labels\n",
            "3708\n",
            "img/59601.png\n",
            "\n",
            "0: 448x640 8 persons, 1 baseball glove, 26.9ms\n",
            "Speed: 2.7ms preprocess, 26.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3708\u001b[0m\n",
            "1 label saved to runs/detect/train3708/labels\n",
            "3709\n",
            "img/49280.png\n",
            "\n",
            "0: 480x640 7 persons, 27.9ms\n",
            "Speed: 2.8ms preprocess, 27.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3709\u001b[0m\n",
            "1 label saved to runs/detect/train3709/labels\n",
            "3710\n",
            "img/60913.png\n",
            "\n",
            "0: 448x640 1 person, 24.0ms\n",
            "Speed: 5.1ms preprocess, 24.0ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3710\u001b[0m\n",
            "1 label saved to runs/detect/train3710/labels\n",
            "3711\n",
            "img/23519.png\n",
            "\n",
            "0: 640x448 1 person, 23.1ms\n",
            "Speed: 3.0ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3711\u001b[0m\n",
            "1 label saved to runs/detect/train3711/labels\n",
            "3712\n",
            "img/79234.png\n",
            "\n",
            "0: 448x640 1 person, 22.7ms\n",
            "Speed: 2.7ms preprocess, 22.7ms inference, 2.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3712\u001b[0m\n",
            "1 label saved to runs/detect/train3712/labels\n",
            "3713\n",
            "img/42178.png\n",
            "\n",
            "0: 448x640 7 persons, 1 umbrella, 1 handbag, 21.7ms\n",
            "Speed: 2.5ms preprocess, 21.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3713\u001b[0m\n",
            "1 label saved to runs/detect/train3713/labels\n",
            "3714\n",
            "img/84692.png\n",
            "\n",
            "0: 416x640 1 person, 1 tie, 31.4ms\n",
            "Speed: 2.9ms preprocess, 31.4ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3714\u001b[0m\n",
            "1 label saved to runs/detect/train3714/labels\n",
            "3715\n",
            "img/73690.png\n",
            "\n",
            "0: 640x416 1 person, 43.9ms\n",
            "Speed: 2.5ms preprocess, 43.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3715\u001b[0m\n",
            "1 label saved to runs/detect/train3715/labels\n",
            "3716\n",
            "img/83206.png\n",
            "\n",
            "0: 640x480 5 persons, 1 tie, 29.0ms\n",
            "Speed: 4.1ms preprocess, 29.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3716\u001b[0m\n",
            "1 label saved to runs/detect/train3716/labels\n",
            "3717\n",
            "img/72361.png\n",
            "\n",
            "0: 640x448 1 person, 28.2ms\n",
            "Speed: 2.4ms preprocess, 28.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3717\u001b[0m\n",
            "1 label saved to runs/detect/train3717/labels\n",
            "3718\n",
            "img/76539.png\n",
            "\n",
            "0: 448x640 4 persons, 27.5ms\n",
            "Speed: 4.5ms preprocess, 27.5ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3718\u001b[0m\n",
            "1 label saved to runs/detect/train3718/labels\n",
            "3719\n",
            "img/10639.png\n",
            "\n",
            "0: 640x448 1 person, 33.1ms\n",
            "Speed: 3.2ms preprocess, 33.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3719\u001b[0m\n",
            "1 label saved to runs/detect/train3719/labels\n",
            "3720\n",
            "img/93428.png\n",
            "\n",
            "0: 448x640 1 person, 3 ties, 26.0ms\n",
            "Speed: 3.1ms preprocess, 26.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3720\u001b[0m\n",
            "1 label saved to runs/detect/train3720/labels\n",
            "3721\n",
            "img/09387.png\n",
            "\n",
            "0: 448x640 1 person, 26.4ms\n",
            "Speed: 3.0ms preprocess, 26.4ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3721\u001b[0m\n",
            "1 label saved to runs/detect/train3721/labels\n",
            "3722\n",
            "img/08196.png\n",
            "\n",
            "0: 416x640 1 person, 30.7ms\n",
            "Speed: 8.1ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3722\u001b[0m\n",
            "1 label saved to runs/detect/train3722/labels\n",
            "3723\n",
            "img/16734.png\n",
            "\n",
            "0: 448x640 2 persons, 25.8ms\n",
            "Speed: 3.0ms preprocess, 25.8ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3723\u001b[0m\n",
            "1 label saved to runs/detect/train3723/labels\n",
            "3724\n",
            "img/70592.png\n",
            "\n",
            "0: 640x320 1 person, 2 buss, 1 backpack, 25.0ms\n",
            "Speed: 2.4ms preprocess, 25.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3724\u001b[0m\n",
            "1 label saved to runs/detect/train3724/labels\n",
            "3725\n",
            "img/05329.png\n",
            "\n",
            "0: 448x640 1 person, 1 sheep, 1 frisbee, 33.6ms\n",
            "Speed: 6.8ms preprocess, 33.6ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3725\u001b[0m\n",
            "1 label saved to runs/detect/train3725/labels\n",
            "3726\n",
            "img/58471.png\n",
            "\n",
            "0: 640x320 3 persons, 1 car, 1 bus, 53.9ms\n",
            "Speed: 2.5ms preprocess, 53.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3726\u001b[0m\n",
            "1 label saved to runs/detect/train3726/labels\n",
            "3727\n",
            "img/64958.png\n",
            "\n",
            "0: 640x640 2 persons, 2 cars, 1 boat, 25.9ms\n",
            "Speed: 6.6ms preprocess, 25.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3727\u001b[0m\n",
            "1 label saved to runs/detect/train3727/labels\n",
            "3728\n",
            "img/81532.png\n",
            "\n",
            "0: 448x640 2 persons, 21.4ms\n",
            "Speed: 2.7ms preprocess, 21.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3728\u001b[0m\n",
            "1 label saved to runs/detect/train3728/labels\n",
            "3729\n",
            "img/38761.png\n",
            "\n",
            "0: 416x640 (no detections), 23.9ms\n",
            "Speed: 2.7ms preprocess, 23.9ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3729\u001b[0m\n",
            "0 label saved to runs/detect/train3729/labels\n",
            "3730\n",
            "img/79068.png\n",
            "\n",
            "0: 480x640 2 persons, 20.7ms\n",
            "Speed: 3.0ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3730\u001b[0m\n",
            "1 label saved to runs/detect/train3730/labels\n",
            "3731\n",
            "img/75624.png\n",
            "\n",
            "0: 448x640 4 persons, 1 couch, 20.7ms\n",
            "Speed: 3.8ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3731\u001b[0m\n",
            "1 label saved to runs/detect/train3731/labels\n",
            "3732\n",
            "img/51602.png\n",
            "\n",
            "0: 640x352 7 persons, 1 tie, 20.0ms\n",
            "Speed: 3.4ms preprocess, 20.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3732\u001b[0m\n",
            "1 label saved to runs/detect/train3732/labels\n",
            "3733\n",
            "img/34107.png\n",
            "\n",
            "0: 448x640 1 donut, 19.3ms\n",
            "Speed: 2.9ms preprocess, 19.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3733\u001b[0m\n",
            "1 label saved to runs/detect/train3733/labels\n",
            "3734\n",
            "img/01497.png\n",
            "\n",
            "0: 448x640 2 persons, 18.5ms\n",
            "Speed: 2.7ms preprocess, 18.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3734\u001b[0m\n",
            "1 label saved to runs/detect/train3734/labels\n",
            "3735\n",
            "img/45216.png\n",
            "\n",
            "0: 448x640 1 person, 1 surfboard, 20.4ms\n",
            "Speed: 3.2ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3735\u001b[0m\n",
            "1 label saved to runs/detect/train3735/labels\n",
            "3736\n",
            "img/94067.png\n",
            "\n",
            "0: 640x448 (no detections), 21.8ms\n",
            "Speed: 3.2ms preprocess, 21.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3736\u001b[0m\n",
            "0 label saved to runs/detect/train3736/labels\n",
            "3737\n",
            "img/06237.png\n",
            "\n",
            "0: 640x448 1 person, 18.9ms\n",
            "Speed: 4.0ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3737\u001b[0m\n",
            "1 label saved to runs/detect/train3737/labels\n",
            "3738\n",
            "img/67492.png\n",
            "\n",
            "0: 640x480 1 person, 19.3ms\n",
            "Speed: 3.4ms preprocess, 19.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3738\u001b[0m\n",
            "1 label saved to runs/detect/train3738/labels\n",
            "3739\n",
            "img/65189.png\n",
            "\n",
            "0: 640x480 1 person, 20.1ms\n",
            "Speed: 3.7ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3739\u001b[0m\n",
            "1 label saved to runs/detect/train3739/labels\n",
            "3740\n",
            "img/54376.png\n",
            "\n",
            "0: 640x448 1 person, 19.1ms\n",
            "Speed: 2.7ms preprocess, 19.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3740\u001b[0m\n",
            "1 label saved to runs/detect/train3740/labels\n",
            "3741\n",
            "img/82139.png\n",
            "\n",
            "0: 352x640 1 cat, 1 dog, 22.9ms\n",
            "Speed: 2.5ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3741\u001b[0m\n",
            "1 label saved to runs/detect/train3741/labels\n",
            "3742\n",
            "img/07452.png\n",
            "\n",
            "0: 416x640 5 airplanes, 1 bird, 22.7ms\n",
            "Speed: 4.0ms preprocess, 22.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3742\u001b[0m\n",
            "1 label saved to runs/detect/train3742/labels\n",
            "3743\n",
            "img/41678.png\n",
            "\n",
            "0: 480x640 6 persons, 4 ties, 21.0ms\n",
            "Speed: 3.4ms preprocess, 21.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3743\u001b[0m\n",
            "1 label saved to runs/detect/train3743/labels\n",
            "3744\n",
            "img/29506.png\n",
            "\n",
            "0: 448x640 1 horse, 1 elephant, 23.7ms\n",
            "Speed: 4.1ms preprocess, 23.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3744\u001b[0m\n",
            "1 label saved to runs/detect/train3744/labels\n",
            "3745\n",
            "img/07413.png\n",
            "\n",
            "0: 448x640 1 person, 19.4ms\n",
            "Speed: 3.3ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3745\u001b[0m\n",
            "1 label saved to runs/detect/train3745/labels\n",
            "3746\n",
            "img/61589.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 17.6ms\n",
            "Speed: 3.9ms preprocess, 17.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3746\u001b[0m\n",
            "1 label saved to runs/detect/train3746/labels\n",
            "3747\n",
            "img/32405.png\n",
            "\n",
            "0: 448x640 5 persons, 1 horse, 1 sheep, 2 cell phones, 18.4ms\n",
            "Speed: 2.8ms preprocess, 18.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3747\u001b[0m\n",
            "1 label saved to runs/detect/train3747/labels\n",
            "3748\n",
            "img/78492.png\n",
            "\n",
            "0: 640x512 7 persons, 1 donut, 22.5ms\n",
            "Speed: 2.9ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3748\u001b[0m\n",
            "1 label saved to runs/detect/train3748/labels\n",
            "3749\n",
            "img/67548.png\n",
            "\n",
            "0: 640x640 1 person, 19.2ms\n",
            "Speed: 4.1ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3749\u001b[0m\n",
            "1 label saved to runs/detect/train3749/labels\n",
            "3750\n",
            "img/41023.png\n",
            "\n",
            "0: 640x576 1 tie, 19.2ms\n",
            "Speed: 3.1ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3750\u001b[0m\n",
            "1 label saved to runs/detect/train3750/labels\n",
            "3751\n",
            "img/16075.png\n",
            "\n",
            "0: 640x512 3 persons, 1 car, 19.5ms\n",
            "Speed: 3.8ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3751\u001b[0m\n",
            "1 label saved to runs/detect/train3751/labels\n",
            "3752\n",
            "img/83914.png\n",
            "\n",
            "0: 448x640 1 person, 20.8ms\n",
            "Speed: 3.3ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3752\u001b[0m\n",
            "1 label saved to runs/detect/train3752/labels\n",
            "3753\n",
            "img/78650.png\n",
            "\n",
            "0: 448x640 1 person, 18.7ms\n",
            "Speed: 3.1ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3753\u001b[0m\n",
            "1 label saved to runs/detect/train3753/labels\n",
            "3754\n",
            "img/92846.png\n",
            "\n",
            "0: 448x640 1 person, 1 horse, 4 backpacks, 1 suitcase, 18.2ms\n",
            "Speed: 3.8ms preprocess, 18.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3754\u001b[0m\n",
            "1 label saved to runs/detect/train3754/labels\n",
            "3755\n",
            "img/61743.png\n",
            "\n",
            "0: 640x480 1 person, 1 dog, 21.3ms\n",
            "Speed: 2.7ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3755\u001b[0m\n",
            "1 label saved to runs/detect/train3755/labels\n",
            "3756\n",
            "img/75648.png\n",
            "\n",
            "0: 448x640 2 persons, 1 suitcase, 21.0ms\n",
            "Speed: 2.5ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3756\u001b[0m\n",
            "1 label saved to runs/detect/train3756/labels\n",
            "3757\n",
            "img/60215.png\n",
            "\n",
            "0: 640x416 1 person, 20.9ms\n",
            "Speed: 2.4ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3757\u001b[0m\n",
            "1 label saved to runs/detect/train3757/labels\n",
            "3758\n",
            "img/40756.png\n",
            "\n",
            "0: 448x640 1 person, 22.8ms\n",
            "Speed: 3.0ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3758\u001b[0m\n",
            "1 label saved to runs/detect/train3758/labels\n",
            "3759\n",
            "img/64753.png\n",
            "\n",
            "0: 480x640 1 person, 19.6ms\n",
            "Speed: 3.5ms preprocess, 19.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3759\u001b[0m\n",
            "1 label saved to runs/detect/train3759/labels\n",
            "3760\n",
            "img/02854.png\n",
            "\n",
            "0: 640x448 1 person, 23.3ms\n",
            "Speed: 2.6ms preprocess, 23.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3760\u001b[0m\n",
            "1 label saved to runs/detect/train3760/labels\n",
            "3761\n",
            "img/52036.png\n",
            "\n",
            "0: 384x640 1 person, 21.2ms\n",
            "Speed: 2.5ms preprocess, 21.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3761\u001b[0m\n",
            "1 label saved to runs/detect/train3761/labels\n",
            "3762\n",
            "img/87412.png\n",
            "\n",
            "0: 448x640 3 persons, 20.4ms\n",
            "Speed: 2.8ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3762\u001b[0m\n",
            "1 label saved to runs/detect/train3762/labels\n",
            "3763\n",
            "img/42987.png\n",
            "\n",
            "0: 640x576 3 persons, 1 tie, 1 bed, 1 tv, 20.5ms\n",
            "Speed: 4.2ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3763\u001b[0m\n",
            "1 label saved to runs/detect/train3763/labels\n",
            "3764\n",
            "img/38714.png\n",
            "\n",
            "0: 640x448 1 person, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3764\u001b[0m\n",
            "1 label saved to runs/detect/train3764/labels\n",
            "3765\n",
            "img/54083.png\n",
            "\n",
            "0: 448x640 6 persons, 1 dog, 21.5ms\n",
            "Speed: 2.6ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3765\u001b[0m\n",
            "1 label saved to runs/detect/train3765/labels\n",
            "3766\n",
            "img/61932.png\n",
            "\n",
            "0: 640x448 2 persons, 21.5ms\n",
            "Speed: 3.0ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3766\u001b[0m\n",
            "1 label saved to runs/detect/train3766/labels\n",
            "3767\n",
            "img/20187.png\n",
            "\n",
            "0: 640x448 (no detections), 19.4ms\n",
            "Speed: 2.8ms preprocess, 19.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3767\u001b[0m\n",
            "0 label saved to runs/detect/train3767/labels\n",
            "3768\n",
            "img/73498.png\n",
            "\n",
            "0: 448x640 1 person, 21.1ms\n",
            "Speed: 2.9ms preprocess, 21.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3768\u001b[0m\n",
            "1 label saved to runs/detect/train3768/labels\n",
            "3769\n",
            "img/71593.png\n",
            "\n",
            "0: 448x640 1 person, 1 cell phone, 20.8ms\n",
            "Speed: 3.4ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3769\u001b[0m\n",
            "1 label saved to runs/detect/train3769/labels\n",
            "3770\n",
            "img/10625.png\n",
            "\n",
            "0: 640x480 1 person, 1 bed, 19.0ms\n",
            "Speed: 3.0ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3770\u001b[0m\n",
            "1 label saved to runs/detect/train3770/labels\n",
            "3771\n",
            "img/15290.png\n",
            "\n",
            "0: 448x640 2 persons, 19.2ms\n",
            "Speed: 3.1ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3771\u001b[0m\n",
            "1 label saved to runs/detect/train3771/labels\n",
            "3772\n",
            "img/15083.png\n",
            "\n",
            "0: 640x544 2 persons, 20.3ms\n",
            "Speed: 3.8ms preprocess, 20.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3772\u001b[0m\n",
            "1 label saved to runs/detect/train3772/labels\n",
            "3773\n",
            "img/14397.png\n",
            "\n",
            "0: 640x480 1 person, 23.7ms\n",
            "Speed: 2.7ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3773\u001b[0m\n",
            "1 label saved to runs/detect/train3773/labels\n",
            "3774\n",
            "img/57984.png\n",
            "\n",
            "0: 640x256 6 persons, 2 ties, 18.8ms\n",
            "Speed: 2.0ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Results saved to \u001b[1mruns/detect/train3774\u001b[0m\n",
            "1 label saved to runs/detect/train3774/labels\n",
            "3775\n",
            "img/08695.png\n",
            "\n",
            "0: 640x544 13 persons, 10 cows, 19.5ms\n",
            "Speed: 4.2ms preprocess, 19.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3775\u001b[0m\n",
            "1 label saved to runs/detect/train3775/labels\n",
            "3776\n",
            "img/39051.png\n",
            "\n",
            "0: 448x640 7 persons, 23.2ms\n",
            "Speed: 3.2ms preprocess, 23.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3776\u001b[0m\n",
            "1 label saved to runs/detect/train3776/labels\n",
            "3777\n",
            "img/58602.png\n",
            "\n",
            "0: 448x640 2 persons, 19.1ms\n",
            "Speed: 3.1ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3777\u001b[0m\n",
            "1 label saved to runs/detect/train3777/labels\n",
            "3778\n",
            "img/71503.png\n",
            "\n",
            "0: 448x640 9 persons, 1 traffic light, 1 tie, 19.7ms\n",
            "Speed: 3.1ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3778\u001b[0m\n",
            "1 label saved to runs/detect/train3778/labels\n",
            "3779\n",
            "img/43265.png\n",
            "\n",
            "0: 352x640 1 person, 18.8ms\n",
            "Speed: 2.6ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3779\u001b[0m\n",
            "1 label saved to runs/detect/train3779/labels\n",
            "3780\n",
            "img/59470.png\n",
            "\n",
            "0: 448x640 2 persons, 19.2ms\n",
            "Speed: 3.1ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3780\u001b[0m\n",
            "1 label saved to runs/detect/train3780/labels\n",
            "3781\n",
            "img/73219.png\n",
            "\n",
            "0: 416x640 2 persons, 1 bottle, 1 chair, 1 microwave, 1 oven, 20.1ms\n",
            "Speed: 2.7ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3781\u001b[0m\n",
            "1 label saved to runs/detect/train3781/labels\n",
            "3782\n",
            "img/46357.png\n",
            "\n",
            "0: 640x480 (no detections), 20.6ms\n",
            "Speed: 2.6ms preprocess, 20.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3782\u001b[0m\n",
            "0 label saved to runs/detect/train3782/labels\n",
            "3783\n",
            "img/09527.png\n",
            "\n",
            "0: 640x416 1 person, 21.2ms\n",
            "Speed: 2.6ms preprocess, 21.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3783\u001b[0m\n",
            "1 label saved to runs/detect/train3783/labels\n",
            "3784\n",
            "img/68259.png\n",
            "\n",
            "0: 640x448 2 persons, 19.8ms\n",
            "Speed: 2.9ms preprocess, 19.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3784\u001b[0m\n",
            "1 label saved to runs/detect/train3784/labels\n",
            "3785\n",
            "img/17354.png\n",
            "\n",
            "0: 640x640 1 person, 1 tie, 19.0ms\n",
            "Speed: 3.5ms preprocess, 19.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3785\u001b[0m\n",
            "1 label saved to runs/detect/train3785/labels\n",
            "3786\n",
            "img/19637.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bowl, 1 couch, 20.8ms\n",
            "Speed: 3.3ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3786\u001b[0m\n",
            "1 label saved to runs/detect/train3786/labels\n",
            "3787\n",
            "img/35287.png\n",
            "\n",
            "0: 640x352 1 scissors, 22.5ms\n",
            "Speed: 2.6ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3787\u001b[0m\n",
            "1 label saved to runs/detect/train3787/labels\n",
            "3788\n",
            "img/80971.png\n",
            "\n",
            "0: 640x480 2 persons, 19.0ms\n",
            "Speed: 4.3ms preprocess, 19.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3788\u001b[0m\n",
            "1 label saved to runs/detect/train3788/labels\n",
            "3789\n",
            "img/50746.png\n",
            "\n",
            "0: 448x640 1 person, 20.9ms\n",
            "Speed: 2.7ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3789\u001b[0m\n",
            "1 label saved to runs/detect/train3789/labels\n",
            "3790\n",
            "img/08693.png\n",
            "\n",
            "0: 480x640 1 dining table, 20.2ms\n",
            "Speed: 2.9ms preprocess, 20.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3790\u001b[0m\n",
            "1 label saved to runs/detect/train3790/labels\n",
            "3791\n",
            "img/56843.png\n",
            "\n",
            "0: 544x640 4 persons, 21.6ms\n",
            "Speed: 3.9ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3791\u001b[0m\n",
            "1 label saved to runs/detect/train3791/labels\n",
            "3792\n",
            "img/01653.png\n",
            "\n",
            "0: 448x640 2 persons, 20.1ms\n",
            "Speed: 2.7ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3792\u001b[0m\n",
            "1 label saved to runs/detect/train3792/labels\n",
            "3793\n",
            "img/97402.png\n",
            "\n",
            "0: 416x640 1 motorcycle, 18.9ms\n",
            "Speed: 2.6ms preprocess, 18.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3793\u001b[0m\n",
            "1 label saved to runs/detect/train3793/labels\n",
            "3794\n",
            "img/96270.png\n",
            "\n",
            "0: 640x512 2 persons, 22.3ms\n",
            "Speed: 4.3ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3794\u001b[0m\n",
            "1 label saved to runs/detect/train3794/labels\n",
            "3795\n",
            "img/21704.png\n",
            "\n",
            "0: 512x640 2 dogs, 18.5ms\n",
            "Speed: 3.9ms preprocess, 18.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3795\u001b[0m\n",
            "1 label saved to runs/detect/train3795/labels\n",
            "3796\n",
            "img/81203.png\n",
            "\n",
            "0: 640x448 2 persons, 23.5ms\n",
            "Speed: 2.9ms preprocess, 23.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3796\u001b[0m\n",
            "1 label saved to runs/detect/train3796/labels\n",
            "3797\n",
            "img/96104.png\n",
            "\n",
            "0: 448x640 3 persons, 20.6ms\n",
            "Speed: 3.6ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3797\u001b[0m\n",
            "1 label saved to runs/detect/train3797/labels\n",
            "3798\n",
            "img/30168.png\n",
            "\n",
            "0: 448x640 3 persons, 1 suitcase, 25.8ms\n",
            "Speed: 3.0ms preprocess, 25.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3798\u001b[0m\n",
            "1 label saved to runs/detect/train3798/labels\n",
            "3799\n",
            "img/73105.png\n",
            "\n",
            "0: 480x640 16 persons, 1 tie, 1 chair, 21.7ms\n",
            "Speed: 3.2ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3799\u001b[0m\n",
            "1 label saved to runs/detect/train3799/labels\n",
            "3800\n",
            "img/86175.png\n",
            "\n",
            "0: 448x640 1 person, 21.1ms\n",
            "Speed: 3.1ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3800\u001b[0m\n",
            "1 label saved to runs/detect/train3800/labels\n",
            "3801\n",
            "img/26940.png\n",
            "\n",
            "0: 640x512 1 person, 6 ties, 18.9ms\n",
            "Speed: 5.7ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3801\u001b[0m\n",
            "1 label saved to runs/detect/train3801/labels\n",
            "3802\n",
            "img/24806.png\n",
            "\n",
            "0: 448x640 1 person, 21.1ms\n",
            "Speed: 3.2ms preprocess, 21.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3802\u001b[0m\n",
            "1 label saved to runs/detect/train3802/labels\n",
            "3803\n",
            "img/23615.png\n",
            "\n",
            "0: 448x640 1 person, 1 dog, 1 bear, 1 cell phone, 19.2ms\n",
            "Speed: 3.2ms preprocess, 19.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3803\u001b[0m\n",
            "1 label saved to runs/detect/train3803/labels\n",
            "3804\n",
            "img/18469.png\n",
            "\n",
            "0: 640x320 2 persons, 21.5ms\n",
            "Speed: 2.6ms preprocess, 21.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3804\u001b[0m\n",
            "1 label saved to runs/detect/train3804/labels\n",
            "3805\n",
            "img/01264.png\n",
            "\n",
            "0: 384x640 2 sheeps, 19.5ms\n",
            "Speed: 3.3ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3805\u001b[0m\n",
            "1 label saved to runs/detect/train3805/labels\n",
            "3806\n",
            "img/97601.png\n",
            "\n",
            "0: 448x640 1 banana, 19.5ms\n",
            "Speed: 3.0ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3806\u001b[0m\n",
            "1 label saved to runs/detect/train3806/labels\n",
            "3807\n",
            "img/43792.png\n",
            "\n",
            "0: 448x640 1 cat, 1 dog, 17.6ms\n",
            "Speed: 3.0ms preprocess, 17.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3807\u001b[0m\n",
            "1 label saved to runs/detect/train3807/labels\n",
            "3808\n",
            "img/10947.png\n",
            "\n",
            "0: 448x640 2 persons, 1 car, 19.5ms\n",
            "Speed: 3.0ms preprocess, 19.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3808\u001b[0m\n",
            "1 label saved to runs/detect/train3808/labels\n",
            "3809\n",
            "img/34710.png\n",
            "\n",
            "0: 640x384 3 persons, 1 dining table, 19.7ms\n",
            "Speed: 2.5ms preprocess, 19.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3809\u001b[0m\n",
            "1 label saved to runs/detect/train3809/labels\n",
            "3810\n",
            "img/84305.png\n",
            "\n",
            "0: 448x640 17 persons, 29.9ms\n",
            "Speed: 3.1ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3810\u001b[0m\n",
            "1 label saved to runs/detect/train3810/labels\n",
            "3811\n",
            "img/79036.png\n",
            "\n",
            "0: 640x608 1 person, 22.7ms\n",
            "Speed: 3.2ms preprocess, 22.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 608)\n",
            "Results saved to \u001b[1mruns/detect/train3811\u001b[0m\n",
            "1 label saved to runs/detect/train3811/labels\n",
            "3812\n",
            "img/62018.png\n",
            "\n",
            "0: 640x416 1 person, 18.2ms\n",
            "Speed: 2.8ms preprocess, 18.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3812\u001b[0m\n",
            "1 label saved to runs/detect/train3812/labels\n",
            "3813\n",
            "img/86142.png\n",
            "\n",
            "0: 448x640 4 persons, 1 potted plant, 21.3ms\n",
            "Speed: 3.0ms preprocess, 21.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3813\u001b[0m\n",
            "1 label saved to runs/detect/train3813/labels\n",
            "3814\n",
            "img/21394.png\n",
            "\n",
            "0: 448x640 1 person, 17.8ms\n",
            "Speed: 3.4ms preprocess, 17.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3814\u001b[0m\n",
            "1 label saved to runs/detect/train3814/labels\n",
            "3815\n",
            "img/94106.png\n",
            "\n",
            "0: 640x480 2 persons, 1 tie, 18.8ms\n",
            "Speed: 3.6ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3815\u001b[0m\n",
            "1 label saved to runs/detect/train3815/labels\n",
            "3816\n",
            "img/39170.png\n",
            "\n",
            "0: 640x576 5 persons, 1 horse, 1 sheep, 1 cow, 19.5ms\n",
            "Speed: 4.5ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3816\u001b[0m\n",
            "1 label saved to runs/detect/train3816/labels\n",
            "3817\n",
            "img/78541.png\n",
            "\n",
            "0: 640x480 1 cat, 1 dog, 1 couch, 1 bed, 22.1ms\n",
            "Speed: 3.2ms preprocess, 22.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3817\u001b[0m\n",
            "1 label saved to runs/detect/train3817/labels\n",
            "3818\n",
            "img/59462.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 20.4ms\n",
            "Speed: 2.9ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3818\u001b[0m\n",
            "1 label saved to runs/detect/train3818/labels\n",
            "3819\n",
            "img/68245.png\n",
            "\n",
            "0: 448x640 1 person, 20.7ms\n",
            "Speed: 3.3ms preprocess, 20.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3819\u001b[0m\n",
            "1 label saved to runs/detect/train3819/labels\n",
            "3820\n",
            "img/85670.png\n",
            "\n",
            "0: 512x640 1 person, 19.6ms\n",
            "Speed: 3.2ms preprocess, 19.6ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3820\u001b[0m\n",
            "1 label saved to runs/detect/train3820/labels\n",
            "3821\n",
            "img/23467.png\n",
            "\n",
            "0: 640x512 1 person, 18.9ms\n",
            "Speed: 2.8ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3821\u001b[0m\n",
            "1 label saved to runs/detect/train3821/labels\n",
            "3822\n",
            "img/16845.png\n",
            "\n",
            "0: 640x448 1 person, 3 cars, 19.1ms\n",
            "Speed: 3.1ms preprocess, 19.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3822\u001b[0m\n",
            "1 label saved to runs/detect/train3822/labels\n",
            "3823\n",
            "img/18524.png\n",
            "\n",
            "0: 448x640 1 person, 1 cat, 1 dog, 18.9ms\n",
            "Speed: 2.9ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3823\u001b[0m\n",
            "1 label saved to runs/detect/train3823/labels\n",
            "3824\n",
            "img/48153.png\n",
            "\n",
            "0: 640x576 (no detections), 20.1ms\n",
            "Speed: 3.1ms preprocess, 20.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3824\u001b[0m\n",
            "0 label saved to runs/detect/train3824/labels\n",
            "3825\n",
            "img/10528.png\n",
            "\n",
            "0: 512x640 2 persons, 19.0ms\n",
            "Speed: 3.4ms preprocess, 19.0ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3825\u001b[0m\n",
            "1 label saved to runs/detect/train3825/labels\n",
            "3826\n",
            "img/75960.png\n",
            "\n",
            "0: 448x640 2 persons, 19.8ms\n",
            "Speed: 2.6ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3826\u001b[0m\n",
            "1 label saved to runs/detect/train3826/labels\n",
            "3827\n",
            "img/02439.png\n",
            "\n",
            "0: 448x640 2 persons, 4 cars, 1 truck, 19.7ms\n",
            "Speed: 4.1ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3827\u001b[0m\n",
            "1 label saved to runs/detect/train3827/labels\n",
            "3828\n",
            "img/32081.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bed, 21.0ms\n",
            "Speed: 4.9ms preprocess, 21.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3828\u001b[0m\n",
            "1 label saved to runs/detect/train3828/labels\n",
            "3829\n",
            "img/76458.png\n",
            "\n",
            "0: 448x640 3 persons, 20.3ms\n",
            "Speed: 4.1ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3829\u001b[0m\n",
            "1 label saved to runs/detect/train3829/labels\n",
            "3830\n",
            "img/90625.png\n",
            "\n",
            "0: 448x640 1 person, 19.3ms\n",
            "Speed: 4.3ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3830\u001b[0m\n",
            "1 label saved to runs/detect/train3830/labels\n",
            "3831\n",
            "img/72956.png\n",
            "\n",
            "0: 640x448 2 persons, 19.9ms\n",
            "Speed: 3.3ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3831\u001b[0m\n",
            "1 label saved to runs/detect/train3831/labels\n",
            "3832\n",
            "img/08531.png\n",
            "\n",
            "0: 448x640 (no detections), 22.1ms\n",
            "Speed: 2.8ms preprocess, 22.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3832\u001b[0m\n",
            "0 label saved to runs/detect/train3832/labels\n",
            "3833\n",
            "img/21769.png\n",
            "\n",
            "0: 480x640 1 person, 19.3ms\n",
            "Speed: 3.1ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3833\u001b[0m\n",
            "1 label saved to runs/detect/train3833/labels\n",
            "3834\n",
            "img/75613.png\n",
            "\n",
            "0: 448x640 (no detections), 33.8ms\n",
            "Speed: 2.9ms preprocess, 33.8ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3834\u001b[0m\n",
            "0 label saved to runs/detect/train3834/labels\n",
            "3835\n",
            "img/54201.png\n",
            "\n",
            "0: 512x640 1 person, 1 car, 1 cup, 19.8ms\n",
            "Speed: 3.6ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3835\u001b[0m\n",
            "1 label saved to runs/detect/train3835/labels\n",
            "3836\n",
            "img/78293.png\n",
            "\n",
            "0: 640x448 1 person, 1 dog, 1 tie, 19.6ms\n",
            "Speed: 2.9ms preprocess, 19.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3836\u001b[0m\n",
            "1 label saved to runs/detect/train3836/labels\n",
            "3837\n",
            "img/07125.png\n",
            "\n",
            "0: 640x352 6 persons, 1 handbag, 20.7ms\n",
            "Speed: 2.4ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3837\u001b[0m\n",
            "1 label saved to runs/detect/train3837/labels\n",
            "3838\n",
            "img/72946.png\n",
            "\n",
            "0: 448x640 1 person, 1 train, 1 truck, 19.0ms\n",
            "Speed: 3.6ms preprocess, 19.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3838\u001b[0m\n",
            "1 label saved to runs/detect/train3838/labels\n",
            "3839\n",
            "img/19824.png\n",
            "\n",
            "0: 640x640 1 umbrella, 19.5ms\n",
            "Speed: 3.3ms preprocess, 19.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3839\u001b[0m\n",
            "1 label saved to runs/detect/train3839/labels\n",
            "3840\n",
            "img/31287.png\n",
            "\n",
            "0: 640x576 3 persons, 19.1ms\n",
            "Speed: 4.7ms preprocess, 19.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3840\u001b[0m\n",
            "1 label saved to runs/detect/train3840/labels\n",
            "3841\n",
            "img/86031.png\n",
            "\n",
            "0: 448x640 3 persons, 1 potted plant, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3841\u001b[0m\n",
            "1 label saved to runs/detect/train3841/labels\n",
            "3842\n",
            "img/23657.png\n",
            "\n",
            "0: 448x640 2 persons, 18.9ms\n",
            "Speed: 3.4ms preprocess, 18.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3842\u001b[0m\n",
            "1 label saved to runs/detect/train3842/labels\n",
            "3843\n",
            "img/82367.png\n",
            "\n",
            "0: 640x448 1 person, 20.6ms\n",
            "Speed: 3.5ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3843\u001b[0m\n",
            "1 label saved to runs/detect/train3843/labels\n",
            "3844\n",
            "img/16903.png\n",
            "\n",
            "0: 640x448 1 person, 19.4ms\n",
            "Speed: 3.4ms preprocess, 19.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3844\u001b[0m\n",
            "1 label saved to runs/detect/train3844/labels\n",
            "3845\n",
            "img/27613.png\n",
            "\n",
            "0: 448x640 4 persons, 1 tie, 17.5ms\n",
            "Speed: 3.3ms preprocess, 17.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3845\u001b[0m\n",
            "1 label saved to runs/detect/train3845/labels\n",
            "3846\n",
            "img/05689.png\n",
            "\n",
            "0: 448x640 (no detections), 28.8ms\n",
            "Speed: 4.8ms preprocess, 28.8ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3846\u001b[0m\n",
            "0 label saved to runs/detect/train3846/labels\n",
            "3847\n",
            "img/91724.png\n",
            "\n",
            "0: 448x640 1 person, 27.3ms\n",
            "Speed: 3.0ms preprocess, 27.3ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3847\u001b[0m\n",
            "1 label saved to runs/detect/train3847/labels\n",
            "3848\n",
            "img/86543.png\n",
            "\n",
            "0: 640x448 1 person, 30.4ms\n",
            "Speed: 2.6ms preprocess, 30.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3848\u001b[0m\n",
            "1 label saved to runs/detect/train3848/labels\n",
            "3849\n",
            "img/24876.png\n",
            "\n",
            "0: 448x640 12 persons, 30.0ms\n",
            "Speed: 3.4ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3849\u001b[0m\n",
            "1 label saved to runs/detect/train3849/labels\n",
            "3850\n",
            "img/98657.png\n",
            "\n",
            "0: 512x640 2 persons, 28.6ms\n",
            "Speed: 3.3ms preprocess, 28.6ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3850\u001b[0m\n",
            "1 label saved to runs/detect/train3850/labels\n",
            "3851\n",
            "img/60789.png\n",
            "\n",
            "0: 640x448 1 person, 37.1ms\n",
            "Speed: 2.9ms preprocess, 37.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3851\u001b[0m\n",
            "1 label saved to runs/detect/train3851/labels\n",
            "3852\n",
            "img/93081.png\n",
            "\n",
            "0: 448x640 3 persons, 1 backpack, 25.6ms\n",
            "Speed: 3.1ms preprocess, 25.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3852\u001b[0m\n",
            "1 label saved to runs/detect/train3852/labels\n",
            "3853\n",
            "img/08462.png\n",
            "\n",
            "0: 448x640 1 person, 22.2ms\n",
            "Speed: 2.4ms preprocess, 22.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3853\u001b[0m\n",
            "1 label saved to runs/detect/train3853/labels\n",
            "3854\n",
            "img/47605.png\n",
            "\n",
            "0: 512x640 1 person, 1 kite, 21.6ms\n",
            "Speed: 3.5ms preprocess, 21.6ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3854\u001b[0m\n",
            "1 label saved to runs/detect/train3854/labels\n",
            "3855\n",
            "img/19634.png\n",
            "\n",
            "0: 640x640 1 person, 22.7ms\n",
            "Speed: 3.4ms preprocess, 22.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3855\u001b[0m\n",
            "1 label saved to runs/detect/train3855/labels\n",
            "3856\n",
            "img/37298.png\n",
            "\n",
            "0: 480x640 1 sheep, 1 cow, 32.2ms\n",
            "Speed: 3.3ms preprocess, 32.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3856\u001b[0m\n",
            "1 label saved to runs/detect/train3856/labels\n",
            "3857\n",
            "img/14892.png\n",
            "\n",
            "0: 448x640 1 person, 21.8ms\n",
            "Speed: 3.3ms preprocess, 21.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3857\u001b[0m\n",
            "1 label saved to runs/detect/train3857/labels\n",
            "3858\n",
            "img/38469.png\n",
            "\n",
            "0: 448x640 1 person, 1 umbrella, 1 tie, 1 chair, 19.8ms\n",
            "Speed: 3.0ms preprocess, 19.8ms inference, 3.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3858\u001b[0m\n",
            "1 label saved to runs/detect/train3858/labels\n",
            "3859\n",
            "img/52918.png\n",
            "\n",
            "0: 448x640 1 person, 23.8ms\n",
            "Speed: 2.8ms preprocess, 23.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3859\u001b[0m\n",
            "1 label saved to runs/detect/train3859/labels\n",
            "3860\n",
            "img/71682.png\n",
            "\n",
            "0: 448x640 10 persons, 22.2ms\n",
            "Speed: 2.6ms preprocess, 22.2ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3860\u001b[0m\n",
            "1 label saved to runs/detect/train3860/labels\n",
            "3861\n",
            "img/95278.png\n",
            "\n",
            "0: 448x640 1 dog, 1 teddy bear, 20.9ms\n",
            "Speed: 2.1ms preprocess, 20.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3861\u001b[0m\n",
            "1 label saved to runs/detect/train3861/labels\n",
            "3862\n",
            "img/37186.png\n",
            "\n",
            "0: 448x640 1 bird, 1 clock, 29.3ms\n",
            "Speed: 4.5ms preprocess, 29.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3862\u001b[0m\n",
            "1 label saved to runs/detect/train3862/labels\n",
            "3863\n",
            "img/57962.png\n",
            "\n",
            "0: 448x640 1 cat, 23.2ms\n",
            "Speed: 4.3ms preprocess, 23.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3863\u001b[0m\n",
            "1 label saved to runs/detect/train3863/labels\n",
            "3864\n",
            "img/09814.png\n",
            "\n",
            "0: 448x640 5 persons, 28.7ms\n",
            "Speed: 3.1ms preprocess, 28.7ms inference, 5.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3864\u001b[0m\n",
            "1 label saved to runs/detect/train3864/labels\n",
            "3865\n",
            "img/90654.png\n",
            "\n",
            "0: 448x640 1 person, 31.7ms\n",
            "Speed: 2.8ms preprocess, 31.7ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3865\u001b[0m\n",
            "1 label saved to runs/detect/train3865/labels\n",
            "3866\n",
            "img/71539.png\n",
            "\n",
            "0: 640x480 2 persons, 2 ties, 26.9ms\n",
            "Speed: 3.7ms preprocess, 26.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3866\u001b[0m\n",
            "1 label saved to runs/detect/train3866/labels\n",
            "3867\n",
            "img/34785.png\n",
            "\n",
            "0: 448x640 13 persons, 30.0ms\n",
            "Speed: 2.7ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3867\u001b[0m\n",
            "1 label saved to runs/detect/train3867/labels\n",
            "3868\n",
            "img/96701.png\n",
            "\n",
            "0: 640x256 2 persons, 24.0ms\n",
            "Speed: 1.8ms preprocess, 24.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Results saved to \u001b[1mruns/detect/train3868\u001b[0m\n",
            "1 label saved to runs/detect/train3868/labels\n",
            "3869\n",
            "img/76239.png\n",
            "\n",
            "0: 544x640 1 person, 1 tie, 30.6ms\n",
            "Speed: 3.7ms preprocess, 30.6ms inference, 3.9ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3869\u001b[0m\n",
            "1 label saved to runs/detect/train3869/labels\n",
            "3870\n",
            "img/58914.png\n",
            "\n",
            "0: 352x640 7 persons, 27.8ms\n",
            "Speed: 2.7ms preprocess, 27.8ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3870\u001b[0m\n",
            "1 label saved to runs/detect/train3870/labels\n",
            "3871\n",
            "img/36982.png\n",
            "\n",
            "0: 448x640 2 persons, 28.3ms\n",
            "Speed: 2.9ms preprocess, 28.3ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3871\u001b[0m\n",
            "1 label saved to runs/detect/train3871/labels\n",
            "3872\n",
            "img/75193.png\n",
            "\n",
            "0: 640x544 1 person, 26.5ms\n",
            "Speed: 3.1ms preprocess, 26.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3872\u001b[0m\n",
            "1 label saved to runs/detect/train3872/labels\n",
            "3873\n",
            "img/09518.png\n",
            "\n",
            "0: 640x256 4 persons, 2 beds, 28.4ms\n",
            "Speed: 2.0ms preprocess, 28.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 256)\n",
            "Results saved to \u001b[1mruns/detect/train3873\u001b[0m\n",
            "1 label saved to runs/detect/train3873/labels\n",
            "3874\n",
            "img/74206.png\n",
            "\n",
            "0: 640x544 6 persons, 26.3ms\n",
            "Speed: 3.2ms preprocess, 26.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3874\u001b[0m\n",
            "1 label saved to runs/detect/train3874/labels\n",
            "3875\n",
            "img/38157.png\n",
            "\n",
            "0: 640x480 1 person, 1 tie, 26.3ms\n",
            "Speed: 4.1ms preprocess, 26.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3875\u001b[0m\n",
            "1 label saved to runs/detect/train3875/labels\n",
            "3876\n",
            "img/20438.png\n",
            "\n",
            "0: 640x448 1 bear, 45.4ms\n",
            "Speed: 3.1ms preprocess, 45.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3876\u001b[0m\n",
            "1 label saved to runs/detect/train3876/labels\n",
            "3877\n",
            "img/69012.png\n",
            "\n",
            "0: 640x448 1 person, 34.9ms\n",
            "Speed: 2.5ms preprocess, 34.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3877\u001b[0m\n",
            "1 label saved to runs/detect/train3877/labels\n",
            "3878\n",
            "img/62391.png\n",
            "\n",
            "0: 640x448 1 person, 30.6ms\n",
            "Speed: 3.1ms preprocess, 30.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3878\u001b[0m\n",
            "1 label saved to runs/detect/train3878/labels\n",
            "3879\n",
            "img/24108.png\n",
            "\n",
            "0: 640x224 2 persons, 1 tie, 1 wine glass, 28.5ms\n",
            "Speed: 1.8ms preprocess, 28.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 224)\n",
            "Results saved to \u001b[1mruns/detect/train3879\u001b[0m\n",
            "1 label saved to runs/detect/train3879/labels\n",
            "3880\n",
            "img/45207.png\n",
            "\n",
            "0: 448x640 1 person, 21.5ms\n",
            "Speed: 3.3ms preprocess, 21.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3880\u001b[0m\n",
            "1 label saved to runs/detect/train3880/labels\n",
            "3881\n",
            "img/36497.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bed, 19.1ms\n",
            "Speed: 3.3ms preprocess, 19.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3881\u001b[0m\n",
            "1 label saved to runs/detect/train3881/labels\n",
            "3882\n",
            "img/98316.png\n",
            "\n",
            "0: 640x320 3 persons, 21.7ms\n",
            "Speed: 2.6ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3882\u001b[0m\n",
            "1 label saved to runs/detect/train3882/labels\n",
            "3883\n",
            "img/47608.png\n",
            "\n",
            "0: 448x640 3 persons, 1 cell phone, 19.4ms\n",
            "Speed: 3.0ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3883\u001b[0m\n",
            "1 label saved to runs/detect/train3883/labels\n",
            "3884\n",
            "img/86104.png\n",
            "\n",
            "0: 640x224 2 persons, 20.8ms\n",
            "Speed: 1.5ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 224)\n",
            "Results saved to \u001b[1mruns/detect/train3884\u001b[0m\n",
            "1 label saved to runs/detect/train3884/labels\n",
            "3885\n",
            "img/03254.png\n",
            "\n",
            "0: 448x640 2 persons, 1 tie, 20.8ms\n",
            "Speed: 2.9ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3885\u001b[0m\n",
            "1 label saved to runs/detect/train3885/labels\n",
            "3886\n",
            "img/73091.png\n",
            "\n",
            "0: 480x640 13 persons, 1 handbag, 20.1ms\n",
            "Speed: 2.9ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3886\u001b[0m\n",
            "1 label saved to runs/detect/train3886/labels\n",
            "3887\n",
            "img/61840.png\n",
            "\n",
            "0: 608x640 9 persons, 1 truck, 27.0ms\n",
            "Speed: 4.5ms preprocess, 27.0ms inference, 2.4ms postprocess per image at shape (1, 3, 608, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3887\u001b[0m\n",
            "1 label saved to runs/detect/train3887/labels\n",
            "3888\n",
            "img/46175.png\n",
            "\n",
            "0: 480x640 2 persons, 19.8ms\n",
            "Speed: 3.4ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3888\u001b[0m\n",
            "1 label saved to runs/detect/train3888/labels\n",
            "3889\n",
            "img/08941.png\n",
            "\n",
            "0: 448x640 1 dog, 20.8ms\n",
            "Speed: 2.7ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3889\u001b[0m\n",
            "1 label saved to runs/detect/train3889/labels\n",
            "3890\n",
            "img/87165.png\n",
            "\n",
            "0: 448x640 1 bird, 1 donut, 25.1ms\n",
            "Speed: 3.0ms preprocess, 25.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3890\u001b[0m\n",
            "1 label saved to runs/detect/train3890/labels\n",
            "3891\n",
            "img/53261.png\n",
            "\n",
            "0: 448x640 10 persons, 1 baseball glove, 22.6ms\n",
            "Speed: 5.1ms preprocess, 22.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3891\u001b[0m\n",
            "1 label saved to runs/detect/train3891/labels\n",
            "3892\n",
            "img/42638.png\n",
            "\n",
            "0: 640x416 10 persons, 1 cow, 21.4ms\n",
            "Speed: 3.8ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3892\u001b[0m\n",
            "1 label saved to runs/detect/train3892/labels\n",
            "3893\n",
            "img/12490.png\n",
            "\n",
            "0: 416x640 1 person, 20.6ms\n",
            "Speed: 3.9ms preprocess, 20.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3893\u001b[0m\n",
            "1 label saved to runs/detect/train3893/labels\n",
            "3894\n",
            "img/65248.png\n",
            "\n",
            "0: 448x640 5 persons, 1 tie, 1 book, 23.0ms\n",
            "Speed: 3.2ms preprocess, 23.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3894\u001b[0m\n",
            "1 label saved to runs/detect/train3894/labels\n",
            "3895\n",
            "img/83549.png\n",
            "\n",
            "0: 544x640 1 person, 1 baseball glove, 19.8ms\n",
            "Speed: 4.0ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3895\u001b[0m\n",
            "1 label saved to runs/detect/train3895/labels\n",
            "3896\n",
            "img/14852.png\n",
            "\n",
            "0: 640x640 1 person, 1 dog, 20.5ms\n",
            "Speed: 6.1ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3896\u001b[0m\n",
            "1 label saved to runs/detect/train3896/labels\n",
            "3897\n",
            "img/41298.png\n",
            "\n",
            "0: 320x640 1 person, 21.8ms\n",
            "Speed: 2.1ms preprocess, 21.8ms inference, 1.7ms postprocess per image at shape (1, 3, 320, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3897\u001b[0m\n",
            "1 label saved to runs/detect/train3897/labels\n",
            "3898\n",
            "img/61204.png\n",
            "\n",
            "0: 448x640 2 persons, 3 dogs, 2 bottles, 2 potted plants, 3 cell phones, 29.5ms\n",
            "Speed: 3.2ms preprocess, 29.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3898\u001b[0m\n",
            "1 label saved to runs/detect/train3898/labels\n",
            "3899\n",
            "img/47506.png\n",
            "\n",
            "0: 640x512 1 person, 1 bird, 1 umbrella, 20.4ms\n",
            "Speed: 2.8ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3899\u001b[0m\n",
            "1 label saved to runs/detect/train3899/labels\n",
            "3900\n",
            "img/93246.png\n",
            "\n",
            "0: 448x640 1 person, 1 cell phone, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3900\u001b[0m\n",
            "1 label saved to runs/detect/train3900/labels\n",
            "3901\n",
            "img/63042.png\n",
            "\n",
            "0: 640x640 (no detections), 19.5ms\n",
            "Speed: 5.2ms preprocess, 19.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3901\u001b[0m\n",
            "0 label saved to runs/detect/train3901/labels\n",
            "3902\n",
            "img/35419.png\n",
            "\n",
            "0: 448x640 1 person, 1 baseball glove, 20.0ms\n",
            "Speed: 3.1ms preprocess, 20.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3902\u001b[0m\n",
            "1 label saved to runs/detect/train3902/labels\n",
            "3903\n",
            "img/91374.png\n",
            "\n",
            "0: 448x640 5 persons, 18.7ms\n",
            "Speed: 2.7ms preprocess, 18.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3903\u001b[0m\n",
            "1 label saved to runs/detect/train3903/labels\n",
            "3904\n",
            "img/83517.png\n",
            "\n",
            "0: 640x480 2 persons, 19.0ms\n",
            "Speed: 3.0ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3904\u001b[0m\n",
            "1 label saved to runs/detect/train3904/labels\n",
            "3905\n",
            "img/10357.png\n",
            "\n",
            "0: 448x640 1 person, 20.5ms\n",
            "Speed: 3.8ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3905\u001b[0m\n",
            "1 label saved to runs/detect/train3905/labels\n",
            "3906\n",
            "img/79018.png\n",
            "\n",
            "0: 448x640 4 donuts, 18.0ms\n",
            "Speed: 2.7ms preprocess, 18.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3906\u001b[0m\n",
            "1 label saved to runs/detect/train3906/labels\n",
            "3907\n",
            "img/27503.png\n",
            "\n",
            "0: 640x480 1 person, 19.8ms\n",
            "Speed: 3.3ms preprocess, 19.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3907\u001b[0m\n",
            "1 label saved to runs/detect/train3907/labels\n",
            "3908\n",
            "img/36850.png\n",
            "\n",
            "0: 640x480 1 person, 21.8ms\n",
            "Speed: 2.7ms preprocess, 21.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3908\u001b[0m\n",
            "1 label saved to runs/detect/train3908/labels\n",
            "3909\n",
            "img/48516.png\n",
            "\n",
            "0: 448x640 11 persons, 22.8ms\n",
            "Speed: 2.9ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3909\u001b[0m\n",
            "1 label saved to runs/detect/train3909/labels\n",
            "3910\n",
            "img/09875.png\n",
            "\n",
            "0: 448x640 2 persons, 1 bed, 19.7ms\n",
            "Speed: 3.5ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3910\u001b[0m\n",
            "1 label saved to runs/detect/train3910/labels\n",
            "3911\n",
            "img/76819.png\n",
            "\n",
            "0: 512x640 1 person, 20.0ms\n",
            "Speed: 3.5ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3911\u001b[0m\n",
            "1 label saved to runs/detect/train3911/labels\n",
            "3912\n",
            "img/76583.png\n",
            "\n",
            "0: 448x640 1 person, 19.2ms\n",
            "Speed: 2.7ms preprocess, 19.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3912\u001b[0m\n",
            "1 label saved to runs/detect/train3912/labels\n",
            "3913\n",
            "img/73104.png\n",
            "\n",
            "0: 640x640 1 bird, 20.7ms\n",
            "Speed: 4.1ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3913\u001b[0m\n",
            "1 label saved to runs/detect/train3913/labels\n",
            "3914\n",
            "img/08162.png\n",
            "\n",
            "0: 512x640 1 person, 20.8ms\n",
            "Speed: 3.4ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3914\u001b[0m\n",
            "1 label saved to runs/detect/train3914/labels\n",
            "3915\n",
            "img/13902.png\n",
            "\n",
            "0: 448x640 1 person, 1 tie, 22.3ms\n",
            "Speed: 5.9ms preprocess, 22.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3915\u001b[0m\n",
            "1 label saved to runs/detect/train3915/labels\n",
            "3916\n",
            "img/79104.png\n",
            "\n",
            "0: 640x512 1 person, 20.8ms\n",
            "Speed: 3.6ms preprocess, 20.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3916\u001b[0m\n",
            "1 label saved to runs/detect/train3916/labels\n",
            "3917\n",
            "img/89356.png\n",
            "\n",
            "0: 448x640 2 persons, 20.2ms\n",
            "Speed: 2.8ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3917\u001b[0m\n",
            "1 label saved to runs/detect/train3917/labels\n",
            "3918\n",
            "img/02581.png\n",
            "\n",
            "0: 640x640 1 person, 19.9ms\n",
            "Speed: 4.0ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3918\u001b[0m\n",
            "1 label saved to runs/detect/train3918/labels\n",
            "3919\n",
            "img/06897.png\n",
            "\n",
            "0: 448x640 1 person, 18.4ms\n",
            "Speed: 4.6ms preprocess, 18.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3919\u001b[0m\n",
            "1 label saved to runs/detect/train3919/labels\n",
            "3920\n",
            "img/90846.png\n",
            "\n",
            "0: 416x640 5 horses, 1 cow, 19.6ms\n",
            "Speed: 2.7ms preprocess, 19.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3920\u001b[0m\n",
            "1 label saved to runs/detect/train3920/labels\n",
            "3921\n",
            "img/81254.png\n",
            "\n",
            "0: 640x544 1 person, 1 tie, 19.7ms\n",
            "Speed: 4.2ms preprocess, 19.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3921\u001b[0m\n",
            "1 label saved to runs/detect/train3921/labels\n",
            "3922\n",
            "img/47250.png\n",
            "\n",
            "0: 448x640 1 person, 20.0ms\n",
            "Speed: 3.3ms preprocess, 20.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3922\u001b[0m\n",
            "1 label saved to runs/detect/train3922/labels\n",
            "3923\n",
            "img/37024.png\n",
            "\n",
            "0: 448x640 2 persons, 2 dogs, 18.0ms\n",
            "Speed: 4.9ms preprocess, 18.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3923\u001b[0m\n",
            "1 label saved to runs/detect/train3923/labels\n",
            "3924\n",
            "img/98726.png\n",
            "\n",
            "0: 480x640 1 person, 20.0ms\n",
            "Speed: 3.3ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3924\u001b[0m\n",
            "1 label saved to runs/detect/train3924/labels\n",
            "3925\n",
            "img/67298.png\n",
            "\n",
            "0: 640x640 1 person, 1 chair, 18.8ms\n",
            "Speed: 3.7ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3925\u001b[0m\n",
            "1 label saved to runs/detect/train3925/labels\n",
            "3926\n",
            "img/52687.png\n",
            "\n",
            "0: 448x640 1 person, 21.3ms\n",
            "Speed: 3.4ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3926\u001b[0m\n",
            "1 label saved to runs/detect/train3926/labels\n",
            "3927\n",
            "img/96238.png\n",
            "\n",
            "0: 640x448 3 persons, 1 tie, 1 cell phone, 19.2ms\n",
            "Speed: 4.2ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3927\u001b[0m\n",
            "1 label saved to runs/detect/train3927/labels\n",
            "3928\n",
            "img/96513.png\n",
            "\n",
            "0: 480x640 2 trucks, 20.3ms\n",
            "Speed: 3.6ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3928\u001b[0m\n",
            "1 label saved to runs/detect/train3928/labels\n",
            "3929\n",
            "img/15043.png\n",
            "\n",
            "0: 512x640 2 persons, 22.2ms\n",
            "Speed: 3.0ms preprocess, 22.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3929\u001b[0m\n",
            "1 label saved to runs/detect/train3929/labels\n",
            "3930\n",
            "img/35794.png\n",
            "\n",
            "0: 640x448 1 person, 19.9ms\n",
            "Speed: 3.4ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3930\u001b[0m\n",
            "1 label saved to runs/detect/train3930/labels\n",
            "3931\n",
            "img/87059.png\n",
            "\n",
            "0: 640x576 (no detections), 19.7ms\n",
            "Speed: 3.2ms preprocess, 19.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3931\u001b[0m\n",
            "0 label saved to runs/detect/train3931/labels\n",
            "3932\n",
            "img/94732.png\n",
            "\n",
            "0: 448x640 1 person, 19.7ms\n",
            "Speed: 2.9ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3932\u001b[0m\n",
            "1 label saved to runs/detect/train3932/labels\n",
            "3933\n",
            "img/63814.png\n",
            "\n",
            "0: 448x640 1 person, 20.2ms\n",
            "Speed: 3.2ms preprocess, 20.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3933\u001b[0m\n",
            "1 label saved to runs/detect/train3933/labels\n",
            "3934\n",
            "img/64287.png\n",
            "\n",
            "0: 448x640 2 persons, 20.7ms\n",
            "Speed: 2.9ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3934\u001b[0m\n",
            "1 label saved to runs/detect/train3934/labels\n",
            "3935\n",
            "img/76435.png\n",
            "\n",
            "0: 448x640 3 persons, 22.2ms\n",
            "Speed: 2.7ms preprocess, 22.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3935\u001b[0m\n",
            "1 label saved to runs/detect/train3935/labels\n",
            "3936\n",
            "img/02185.png\n",
            "\n",
            "0: 448x640 1 person, 18.6ms\n",
            "Speed: 3.5ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3936\u001b[0m\n",
            "1 label saved to runs/detect/train3936/labels\n",
            "3937\n",
            "img/07834.png\n",
            "\n",
            "0: 480x640 1 person, 1 bed, 18.2ms\n",
            "Speed: 4.0ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3937\u001b[0m\n",
            "1 label saved to runs/detect/train3937/labels\n",
            "3938\n",
            "img/78325.png\n",
            "\n",
            "0: 384x640 1 person, 20.5ms\n",
            "Speed: 3.2ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3938\u001b[0m\n",
            "1 label saved to runs/detect/train3938/labels\n",
            "3939\n",
            "img/70842.png\n",
            "\n",
            "0: 448x640 11 persons, 20.8ms\n",
            "Speed: 2.7ms preprocess, 20.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3939\u001b[0m\n",
            "1 label saved to runs/detect/train3939/labels\n",
            "3940\n",
            "img/03472.png\n",
            "\n",
            "0: 640x448 1 person, 1 bed, 20.7ms\n",
            "Speed: 2.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3940\u001b[0m\n",
            "1 label saved to runs/detect/train3940/labels\n",
            "3941\n",
            "img/32867.png\n",
            "\n",
            "0: 448x640 2 persons, 19.9ms\n",
            "Speed: 4.6ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3941\u001b[0m\n",
            "1 label saved to runs/detect/train3941/labels\n",
            "3942\n",
            "img/34871.png\n",
            "\n",
            "0: 448x640 1 person, 2 ties, 22.5ms\n",
            "Speed: 3.2ms preprocess, 22.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3942\u001b[0m\n",
            "1 label saved to runs/detect/train3942/labels\n",
            "3943\n",
            "img/78034.png\n",
            "\n",
            "0: 512x640 5 persons, 19.9ms\n",
            "Speed: 3.0ms preprocess, 19.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3943\u001b[0m\n",
            "1 label saved to runs/detect/train3943/labels\n",
            "3944\n",
            "img/29438.png\n",
            "\n",
            "0: 640x640 1 cat, 18.2ms\n",
            "Speed: 5.2ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3944\u001b[0m\n",
            "1 label saved to runs/detect/train3944/labels\n",
            "3945\n",
            "img/74192.png\n",
            "\n",
            "0: 640x480 2 persons, 1 cup, 2 chairs, 2 laptops, 30.4ms\n",
            "Speed: 2.7ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3945\u001b[0m\n",
            "1 label saved to runs/detect/train3945/labels\n",
            "3946\n",
            "img/65043.png\n",
            "\n",
            "0: 448x640 1 person, 1 motorcycle, 20.3ms\n",
            "Speed: 2.5ms preprocess, 20.3ms inference, 3.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3946\u001b[0m\n",
            "1 label saved to runs/detect/train3946/labels\n",
            "3947\n",
            "img/63957.png\n",
            "\n",
            "0: 448x640 1 person, 18.6ms\n",
            "Speed: 2.8ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3947\u001b[0m\n",
            "1 label saved to runs/detect/train3947/labels\n",
            "3948\n",
            "img/07218.png\n",
            "\n",
            "0: 640x448 2 persons, 20.0ms\n",
            "Speed: 2.9ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3948\u001b[0m\n",
            "1 label saved to runs/detect/train3948/labels\n",
            "3949\n",
            "img/61082.png\n",
            "\n",
            "0: 640x544 2 persons, 1 cell phone, 25.9ms\n",
            "Speed: 7.6ms preprocess, 25.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3949\u001b[0m\n",
            "1 label saved to runs/detect/train3949/labels\n",
            "3950\n",
            "img/30567.png\n",
            "\n",
            "0: 512x640 4 persons, 1 traffic light, 21.6ms\n",
            "Speed: 3.5ms preprocess, 21.6ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3950\u001b[0m\n",
            "1 label saved to runs/detect/train3950/labels\n",
            "3951\n",
            "img/17290.png\n",
            "\n",
            "0: 448x640 1 person, 21.1ms\n",
            "Speed: 2.9ms preprocess, 21.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3951\u001b[0m\n",
            "1 label saved to runs/detect/train3951/labels\n",
            "3952\n",
            "img/67132.png\n",
            "\n",
            "0: 384x640 3 persons, 22.8ms\n",
            "Speed: 2.8ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3952\u001b[0m\n",
            "1 label saved to runs/detect/train3952/labels\n",
            "3953\n",
            "img/56401.png\n",
            "\n",
            "0: 448x640 (no detections), 19.4ms\n",
            "Speed: 2.8ms preprocess, 19.4ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3953\u001b[0m\n",
            "0 label saved to runs/detect/train3953/labels\n",
            "3954\n",
            "img/57084.png\n",
            "\n",
            "0: 640x576 2 persons, 20.1ms\n",
            "Speed: 4.1ms preprocess, 20.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3954\u001b[0m\n",
            "1 label saved to runs/detect/train3954/labels\n",
            "3955\n",
            "img/79846.png\n",
            "\n",
            "0: 480x640 1 person, 2 cars, 22.9ms\n",
            "Speed: 3.5ms preprocess, 22.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3955\u001b[0m\n",
            "1 label saved to runs/detect/train3955/labels\n",
            "3956\n",
            "img/94638.png\n",
            "\n",
            "0: 640x480 1 person, 21.1ms\n",
            "Speed: 3.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3956\u001b[0m\n",
            "1 label saved to runs/detect/train3956/labels\n",
            "3957\n",
            "img/67051.png\n",
            "\n",
            "0: 640x480 3 persons, 1 tie, 19.5ms\n",
            "Speed: 3.5ms preprocess, 19.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3957\u001b[0m\n",
            "1 label saved to runs/detect/train3957/labels\n",
            "3958\n",
            "img/20867.png\n",
            "\n",
            "0: 640x480 1 dog, 30.2ms\n",
            "Speed: 2.9ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3958\u001b[0m\n",
            "1 label saved to runs/detect/train3958/labels\n",
            "3959\n",
            "img/29470.png\n",
            "\n",
            "0: 640x480 5 persons, 18.7ms\n",
            "Speed: 3.2ms preprocess, 18.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3959\u001b[0m\n",
            "1 label saved to runs/detect/train3959/labels\n",
            "3960\n",
            "img/47359.png\n",
            "\n",
            "0: 448x640 11 persons, 1 car, 1 baseball glove, 20.9ms\n",
            "Speed: 3.7ms preprocess, 20.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3960\u001b[0m\n",
            "1 label saved to runs/detect/train3960/labels\n",
            "3961\n",
            "img/73819.png\n",
            "\n",
            "0: 640x576 2 persons, 1 cup, 18.9ms\n",
            "Speed: 3.8ms preprocess, 18.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3961\u001b[0m\n",
            "1 label saved to runs/detect/train3961/labels\n",
            "3962\n",
            "img/57180.png\n",
            "\n",
            "0: 448x640 3 persons, 1 car, 1 handbag, 1 tie, 19.5ms\n",
            "Speed: 4.7ms preprocess, 19.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3962\u001b[0m\n",
            "1 label saved to runs/detect/train3962/labels\n",
            "3963\n",
            "img/90827.png\n",
            "\n",
            "0: 608x640 2 persons, 1 handbag, 1 frisbee, 19.1ms\n",
            "Speed: 4.1ms preprocess, 19.1ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3963\u001b[0m\n",
            "1 label saved to runs/detect/train3963/labels\n",
            "3964\n",
            "img/38961.png\n",
            "\n",
            "0: 448x640 1 person, 2 umbrellas, 1 tie, 18.2ms\n",
            "Speed: 3.8ms preprocess, 18.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3964\u001b[0m\n",
            "1 label saved to runs/detect/train3964/labels\n",
            "3965\n",
            "img/46538.png\n",
            "\n",
            "0: 448x640 2 persons, 19.3ms\n",
            "Speed: 3.0ms preprocess, 19.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3965\u001b[0m\n",
            "1 label saved to runs/detect/train3965/labels\n",
            "3966\n",
            "img/19547.png\n",
            "\n",
            "0: 640x320 5 persons, 20.3ms\n",
            "Speed: 2.3ms preprocess, 20.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
            "Results saved to \u001b[1mruns/detect/train3966\u001b[0m\n",
            "1 label saved to runs/detect/train3966/labels\n",
            "3967\n",
            "img/84502.png\n",
            "\n",
            "0: 448x640 1 cat, 18.4ms\n",
            "Speed: 3.0ms preprocess, 18.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3967\u001b[0m\n",
            "1 label saved to runs/detect/train3967/labels\n",
            "3968\n",
            "img/71492.png\n",
            "\n",
            "0: 448x640 1 person, 21.3ms\n",
            "Speed: 3.0ms preprocess, 21.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3968\u001b[0m\n",
            "1 label saved to runs/detect/train3968/labels\n",
            "3969\n",
            "img/42105.png\n",
            "\n",
            "0: 544x640 1 person, 20.3ms\n",
            "Speed: 4.0ms preprocess, 20.3ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3969\u001b[0m\n",
            "1 label saved to runs/detect/train3969/labels\n",
            "3970\n",
            "img/38419.png\n",
            "\n",
            "0: 448x640 1 person, 21.4ms\n",
            "Speed: 3.5ms preprocess, 21.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3970\u001b[0m\n",
            "1 label saved to runs/detect/train3970/labels\n",
            "3971\n",
            "img/26057.png\n",
            "\n",
            "0: 448x640 1 person, 21.2ms\n",
            "Speed: 3.0ms preprocess, 21.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3971\u001b[0m\n",
            "1 label saved to runs/detect/train3971/labels\n",
            "3972\n",
            "img/91674.png\n",
            "\n",
            "0: 448x640 1 person, 17.1ms\n",
            "Speed: 3.1ms preprocess, 17.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3972\u001b[0m\n",
            "1 label saved to runs/detect/train3972/labels\n",
            "3973\n",
            "img/41092.png\n",
            "\n",
            "0: 640x480 2 persons, 1 cake, 2 chairs, 1 dining table, 20.5ms\n",
            "Speed: 4.3ms preprocess, 20.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3973\u001b[0m\n",
            "1 label saved to runs/detect/train3973/labels\n",
            "3974\n",
            "img/16892.png\n",
            "\n",
            "0: 640x448 6 persons, 17.9ms\n",
            "Speed: 2.9ms preprocess, 17.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3974\u001b[0m\n",
            "1 label saved to runs/detect/train3974/labels\n",
            "3975\n",
            "img/50124.png\n",
            "\n",
            "0: 640x416 1 person, 20.2ms\n",
            "Speed: 2.5ms preprocess, 20.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3975\u001b[0m\n",
            "1 label saved to runs/detect/train3975/labels\n",
            "3976\n",
            "img/37649.png\n",
            "\n",
            "0: 512x640 2 persons, 18.6ms\n",
            "Speed: 3.2ms preprocess, 18.6ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3976\u001b[0m\n",
            "1 label saved to runs/detect/train3976/labels\n",
            "3977\n",
            "img/34567.png\n",
            "\n",
            "0: 448x640 3 persons, 2 bicycles, 2 cars, 3 cups, 19.4ms\n",
            "Speed: 2.6ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3977\u001b[0m\n",
            "1 label saved to runs/detect/train3977/labels\n",
            "3978\n",
            "img/69450.png\n",
            "\n",
            "0: 640x384 1 cat, 1 dog, 2 cows, 20.0ms\n",
            "Speed: 3.0ms preprocess, 20.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1mruns/detect/train3978\u001b[0m\n",
            "1 label saved to runs/detect/train3978/labels\n",
            "3979\n",
            "img/82960.png\n",
            "\n",
            "0: 416x640 (no detections), 18.6ms\n",
            "Speed: 2.6ms preprocess, 18.6ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3979\u001b[0m\n",
            "0 label saved to runs/detect/train3979/labels\n",
            "3980\n",
            "img/76038.png\n",
            "\n",
            "0: 640x448 1 person, 19.8ms\n",
            "Speed: 2.5ms preprocess, 19.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3980\u001b[0m\n",
            "1 label saved to runs/detect/train3980/labels\n",
            "3981\n",
            "img/94068.png\n",
            "\n",
            "0: 448x640 1 person, 20.7ms\n",
            "Speed: 2.8ms preprocess, 20.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3981\u001b[0m\n",
            "1 label saved to runs/detect/train3981/labels\n",
            "3982\n",
            "img/86750.png\n",
            "\n",
            "0: 640x576 4 persons, 1 tie, 18.9ms\n",
            "Speed: 3.7ms preprocess, 18.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "Results saved to \u001b[1mruns/detect/train3982\u001b[0m\n",
            "1 label saved to runs/detect/train3982/labels\n",
            "3983\n",
            "img/84297.png\n",
            "\n",
            "0: 640x512 1 person, 19.7ms\n",
            "Speed: 3.9ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
            "Results saved to \u001b[1mruns/detect/train3983\u001b[0m\n",
            "1 label saved to runs/detect/train3983/labels\n",
            "3984\n",
            "img/87054.png\n",
            "\n",
            "0: 640x608 1 person, 1 tie, 19.8ms\n",
            "Speed: 3.6ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
            "Results saved to \u001b[1mruns/detect/train3984\u001b[0m\n",
            "1 label saved to runs/detect/train3984/labels\n",
            "3985\n",
            "img/31278.png\n",
            "\n",
            "0: 640x544 1 person, 19.7ms\n",
            "Speed: 3.4ms preprocess, 19.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
            "Results saved to \u001b[1mruns/detect/train3985\u001b[0m\n",
            "1 label saved to runs/detect/train3985/labels\n",
            "3986\n",
            "img/10693.png\n",
            "\n",
            "0: 640x416 3 persons, 20.1ms\n",
            "Speed: 3.3ms preprocess, 20.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
            "Results saved to \u001b[1mruns/detect/train3986\u001b[0m\n",
            "1 label saved to runs/detect/train3986/labels\n",
            "3987\n",
            "img/36748.png\n",
            "\n",
            "0: 448x640 2 persons, 2 couchs, 1 dining table, 1 laptop, 20.4ms\n",
            "Speed: 2.7ms preprocess, 20.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3987\u001b[0m\n",
            "1 label saved to runs/detect/train3987/labels\n",
            "3988\n",
            "img/13596.png\n",
            "\n",
            "0: 640x448 6 persons, 1 suitcase, 1 chair, 18.1ms\n",
            "Speed: 3.1ms preprocess, 18.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3988\u001b[0m\n",
            "1 label saved to runs/detect/train3988/labels\n",
            "3989\n",
            "img/63741.png\n",
            "\n",
            "0: 640x480 10 persons, 21.0ms\n",
            "Speed: 2.9ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3989\u001b[0m\n",
            "1 label saved to runs/detect/train3989/labels\n",
            "3990\n",
            "img/39852.png\n",
            "\n",
            "0: 480x640 3 persons, 20.1ms\n",
            "Speed: 3.1ms preprocess, 20.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3990\u001b[0m\n",
            "1 label saved to runs/detect/train3990/labels\n",
            "3991\n",
            "img/29735.png\n",
            "\n",
            "0: 448x640 2 persons, 1 cell phone, 19.6ms\n",
            "Speed: 2.5ms preprocess, 19.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3991\u001b[0m\n",
            "1 label saved to runs/detect/train3991/labels\n",
            "3992\n",
            "img/91476.png\n",
            "\n",
            "0: 640x448 1 person, 21.0ms\n",
            "Speed: 2.4ms preprocess, 21.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3992\u001b[0m\n",
            "1 label saved to runs/detect/train3992/labels\n",
            "3993\n",
            "img/69305.png\n",
            "\n",
            "0: 640x640 1 motorcycle, 18.8ms\n",
            "Speed: 3.9ms preprocess, 18.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3993\u001b[0m\n",
            "1 label saved to runs/detect/train3993/labels\n",
            "3994\n",
            "img/65073.png\n",
            "\n",
            "0: 448x640 1 person, 19.4ms\n",
            "Speed: 3.6ms preprocess, 19.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3994\u001b[0m\n",
            "1 label saved to runs/detect/train3994/labels\n",
            "3995\n",
            "img/15603.png\n",
            "\n",
            "0: 640x352 2 persons, 1 tie, 22.8ms\n",
            "Speed: 2.8ms preprocess, 22.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
            "Results saved to \u001b[1mruns/detect/train3995\u001b[0m\n",
            "1 label saved to runs/detect/train3995/labels\n",
            "3996\n",
            "img/90318.png\n",
            "\n",
            "0: 448x640 1 person, 32.3ms\n",
            "Speed: 3.2ms preprocess, 32.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3996\u001b[0m\n",
            "1 label saved to runs/detect/train3996/labels\n",
            "3997\n",
            "img/72536.png\n",
            "\n",
            "0: 640x448 1 person, 1 tie, 29.5ms\n",
            "Speed: 3.4ms preprocess, 29.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "Results saved to \u001b[1mruns/detect/train3997\u001b[0m\n",
            "1 label saved to runs/detect/train3997/labels\n",
            "3998\n",
            "img/31924.png\n",
            "\n",
            "0: 640x480 19 persons, 23.9ms\n",
            "Speed: 2.5ms preprocess, 23.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mruns/detect/train3998\u001b[0m\n",
            "1 label saved to runs/detect/train3998/labels\n",
            "3999\n",
            "img/89602.png\n",
            "\n",
            "0: 384x640 1 person, 1 cake, 2 chairs, 1 bed, 1 dining table, 25.4ms\n",
            "Speed: 3.1ms preprocess, 25.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/train3999\u001b[0m\n",
            "1 label saved to runs/detect/train3999/labels\n",
            "4000\n",
            "img/19537.png\n",
            "\n",
            "0: 448x640 1 cat, 22.1ms\n",
            "Speed: 2.8ms preprocess, 22.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/train4000\u001b[0m\n",
            "1 label saved to runs/detect/train4000/labels\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "write() argument must be str, not dict",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-405703f52624>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../freqs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(r'runs')"
      ],
      "metadata": {
        "id": "6TUE_RasenjW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gt-1YXjiueH",
        "outputId": "31fc21e9-66db-4958-b746-5ed653833f2a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'person': 8584, 'tie': 636, 'cat': 94, 'dog': 269, 'frisbee': 17, 'remote': 41, 'baseball glove': 20, 'cow': 131, 'cell phone': 188, 'couch': 70, 'tv': 30, 'umbrella': 40, 'handbag': 80, 'bus': 18, 'car': 236, 'bird': 68, 'cake': 37, 'bowl': 103, 'bottle': 127, 'orange': 27, 'apple': 14, 'cup': 149, 'potted plant': 45, 'chair': 262, 'scissors': 19, 'airplane': 17, 'clock': 24, 'book': 29, 'suitcase': 27, 'pizza': 16, 'dining table': 73, 'spoon': 22, 'knife': 12, 'oven': 45, 'donut': 56, 'surfboard': 47, 'bed': 88, 'traffic light': 30, 'banana': 29, 'wine glass': 33, 'horse': 104, 'skateboard': 9, 'skis': 1, 'teddy bear': 21, 'kite': 16, 'laptop': 43, 'truck': 61, 'backpack': 61, 'hot dog': 49, 'sink': 14, 'toilet': 7, 'train': 13, 'bear': 10, 'motorcycle': 45, 'sports ball': 33, 'elephant': 8, 'sheep': 92, 'giraffe': 8, 'bicycle': 22, 'stop sign': 6, 'mouse': 4, 'microwave': 16, 'tennis racket': 9, 'refrigerator': 16, 'toothbrush': 13, 'vase': 12, 'baseball bat': 19, 'boat': 24, 'bench': 13, 'keyboard': 4, 'sandwich': 11, 'parking meter': 5, 'carrot': 8, 'snowboard': 3, 'fire hydrant': 1, 'fork': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "total_occurrences = sum(freqs.values())\n",
        "\n",
        "frequency = {label: count / total_occurrences for label, count in freqs.items()}\n",
        "\n",
        "most_common_objects = sorted(freqs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"Most common objects:\")\n",
        "for label, count in most_common_objects:\n",
        "    print(f\"{label}: {count} occurrences\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.bar(freqs.keys(), freqs.values())\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.xlabel('Object Label')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency Distribution of Object Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "id": "wiQzRJFkjJY3",
        "outputId": "191d40ac-20b2-414c-ba6b-f1abbedc5e5a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Most common objects:\n",
            "person: 8584 occurrences\n",
            "tie: 636 occurrences\n",
            "dog: 269 occurrences\n",
            "chair: 262 occurrences\n",
            "car: 236 occurrences\n",
            "cell phone: 188 occurrences\n",
            "cup: 149 occurrences\n",
            "cow: 131 occurrences\n",
            "bottle: 127 occurrences\n",
            "horse: 104 occurrences\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}